{
  
    
        "post0": {
            "title": "(5주차) 10월5일 -- draft",
            "content": "imports . import torch import pandas as pd import numpy as np import matplotlib.pyplot as plt . &#49884;&#44033;&#54868;&#47484; &#50948;&#54620; &#51456;&#48708;&#54632;&#49688;&#46308; . 준비1 loss_fn을 plot하는 함수 . def plot_loss(loss_fn,ax=None): if ax==None: fig = plt.figure() ax=fig.add_subplot(1,1,1,projection=&#39;3d&#39;) ax.elev=15;ax.azim=75 w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.15),torch.arange(-1,10,0.15),indexing=&#39;ij&#39;) w0hat = w0hat.reshape(-1) w1hat = w1hat.reshape(-1) def l(w0hat,w1hat): yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x)) return loss_fn(yhat,y) loss = list(map(l,w0hat,w1hat)) ax.scatter(w0hat,w1hat,loss,s=0.1,alpha=0.2) ax.scatter(-1,5,l(-1,5),s=200,marker=&#39;*&#39;) # 실제로 -1,5에서 최소값을 가지는건 아님.. . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(-1+5x_i)}{1+ exp(-1+5x_i)}$ 에서 생성된 데이터 한정하여 손실함수가 그려지게 되어있음. | . 준비2: for문 대신 돌려주고 epoch마다 필요한 정보를 기록하는 함수 . def learn_and_record(net, loss_fn, optimizr): yhat_history = [] loss_history = [] what_history = [] for epoc in range(1000): ## step1 yhat = net(x) ## step2 loss = loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() ## record if epoc % 20 ==0: yhat_history.append(yhat.reshape(-1).data.tolist()) loss_history.append(loss.item()) what_history.append([net[0].bias.data.item(), net[0].weight.data.item()]) return yhat_history, loss_history, what_history . 20에폭마다 yhat, loss, what을 기록 | . 준비3: 애니메이션을 만들어주는 함수 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . def show_lrpr2(net,loss_fn,optimizr,suptitle=&#39;&#39;): yhat_history,loss_history,what_history = learn_and_record(net,loss_fn,optimizr) fig = plt.figure(figsize=(7,2.5)) ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ax1.set_xticks([]);ax1.set_yticks([]) ax2.set_xticks([]);ax2.set_yticks([]);ax2.set_zticks([]) ax2.elev = 15; ax2.azim = 75 ## ax1: 왼쪽그림 ax1.plot(x,v,&#39;--&#39;) ax1.scatter(x,y,alpha=0.05) line, = ax1.plot(x,yhat_history[0],&#39;--&#39;) plot_loss(loss_fn,ax2) fig.suptitle(suptitle) fig.tight_layout() def animate(epoc): line.set_ydata(yhat_history[epoc]) ax2.scatter(np.array(what_history)[epoc,0],np.array(what_history)[epoc,1],loss_history[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() return ani . 준비1에서 그려진 loss 함수위에, 준비2의 정보를 조합하여 애니메이션을 만들어주는 함수 | . Logistic intro (review + $ alpha$) . - 모델: $x$가 커질수록 $y=1$이 잘나오는 모형은 아래와 같이 설계할 수 있음 &lt; 외우세요!!! . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+ exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss= - sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ &lt; 외우세요!! . | . - toy example . x=torch.linspace(-1,1,2000).reshape(2000,1) w0= -1 w1= 5 u = w0+x*w1 v = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함 y = torch.bernoulli(v) . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4feed234d0&gt;] . note: $(w_0,w_1)$의 true는 $(-1,5)$이다. -&gt; $( hat{w}_0, hat{w}_1)$을 적당히 $(-1,5)$근처로 추정하면 된다는 의미 | . plt.scatter(x,y,alpha=0.05) plt.plot(x,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fee40ecd0&gt;] . - step1: yhat을 만들기 . (방법1) . torch.manual_seed(43052) l1 = torch.nn.Linear(1,1) #x의 shape보면(2000,1)인데 뒤에 1이 중요.. a1 = torch.nn.Sigmoid() yhat = a1(l1(x)) yhat . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . (방법2) . torch.manual_seed(43052) l1 = torch.nn.Linear(1,1) a1 = torch.nn.Sigmoid() net = torch.nn.Sequential(l1,a1) #net는 l1과 a1의 합성함수 yhat = net(x) yhat . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . (방법3) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) yhat = net(x) yhat # 단점: a1과 l1 각 통과하는게 궁금한데 이건 중간과정 보기가힘들다. # len(net) = 2 : 2가 나오네.. 우너소에 접근을 해보자 #net[0], net[1] . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . net[0] . Linear(in_features=1, out_features=1, bias=True) . net[0](x) . tensor([[-0.5003], [-0.5007], [-0.5010], ..., [-1.1930], [-1.1934], [-1.1937]], grad_fn=&lt;AddmmBackward0&gt;) . net[1] #a1의 기능 . Sigmoid() . net[1](net[0](x)) . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . - step2: loss (일단 MSE로..) . (방법1) . torch.mean((y-yhat)**2) #mse는 교수님들이 싫어한데.. 왜? 몰라 일단 걍 해보쟈 . tensor(0.2846, grad_fn=&lt;MeanBackward0&gt;) . loss=torch.mean((y-yhat)**2) loss . tensor(0.2846, grad_fn=&lt;MeanBackward0&gt;) . (방법2) . loss_fn = torch.nn.MSELoss() loss_fn(yhat,y) # yhat을 먼저쓰자! . tensor(0.2846, grad_fn=&lt;MseLossBackward0&gt;) . - step3~4는 동일 . - 반복 (준비+for문) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=1,bias=True), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() #MSELoss로 하면.. .. 별로? BCE이거로바꾸기 optimizr = torch.optim.SGD(net.parameters(),lr=0.01) . plt.plot(x,y,&#39;o&#39;,alpha=0.01) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fec4d8090&gt;] . for epoc in range(3000): ## step1 yhat = net(x) ## step2 loss = loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() #청소 . plt.plot(x,y,&#39;o&#39;,alpha=0.05) plt.plot(x,v,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fec459e10&gt;] . &#47196;&#51648;&#49828;&#54001;--BCEloss . - BCEloss로 바꾸어서 적합하여 보자. . net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=1,bias=True), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.01) . for epoc in range(3000): ## step1 yhat = net(x) ## step2 loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) # loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.05) plt.plot(x,v,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fec2cb7d0&gt;] . - 왜 잘맞지? -&gt; &quot;linear -&gt; sigmoid&quot; 와 같은 net에 BCEloss를 이용하면 손실함수의 모양이 convex 하기 때문에 . . plot_loss 함수소개 = 이 예제에 한정하여 $ hat{w}_0, hat{w}_1,loss( hat{w}_0, hat{w}_1)$를 각각 $x,y,z$ 축에 그려줍니다. | . plot_loss(torch.nn.MSELoss()) . plot_loss(torch.nn.BCELoss()) . &#49884;&#44033;&#54868;1: MSE, &#51339;&#51008;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) #학습률 . l1,a1 = net #초기값 세팅 l1.bias.data = torch.tensor([-3.0]) #세팅값 l1.weight.data = torch.tensor([[-1.0]]) #세팅값 . show_lrpr2(net,loss_fn,optimizr,&#39;MSEloss, good_init // SGD&#39;) . &lt;/input&gt; Once Loop Reflect &#49884;&#44033;&#54868;2: MSE, &#45208;&#49244;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) . l1,a1 = net l1.bias.data = torch.tensor([-10.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;MSEloss, bad_init // SGD&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;3: BCE, &#51339;&#51008;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) . l1,a1 = net l1.bias.data = torch.tensor([-3.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;BCEloss, good_init // SGD&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;4: BCE, &#45208;&#49244;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) . l1,a1 = net l1.bias.data = torch.tensor([-3.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;BCEloss, good_init // SGD&#39;) . Once Loop Reflect &#47196;&#51648;&#49828;&#54001;--Adam (&#44397;&#48124;&#50741;&#54000;&#47560;&#51060;&#51200;) . # 1. 어려워서 몰라도 뎀 # 2. 가속도의 개념 . &#49884;&#44033;&#54868;1: MSE, &#51339;&#51008;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters(),lr=0.05) ## &lt;-- 여기를 수정! . l1,a1 = net l1.bias.data = torch.tensor([-3.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;MSEloss, good_init // Adam&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;2: MSE, &#45208;&#49244;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters(),lr=0.05) . l1,a1 = net l1.bias.data = torch.tensor([-10.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;MSEloss, bad_init // Adam&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;3: BCE, &#51339;&#51008;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! (&#54844;&#51088;&#54644;&#48400;&#50836;..) . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters(),lr=0.05) . l1,a1 = net l1.bias.data = torch.tensor([-3.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;BCEloss, good_init // Adam&#39;) . &lt;/input&gt; Once Loop Reflect &#49884;&#44033;&#54868;4: BCE, &#45208;&#49244;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! (&#54844;&#51088;&#54644;&#48400;&#50836;..) . (참고) Adam이 우수한 이유? SGD보다 두 가지 측면에서 개선이 있었음. . 그런게 있음.. | 가속도의 개념을 적용!! | &#44618;&#51008;&#49888;&#44221;&#47581;--&#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#51032; &#54620;&#44228; . &#49888;&#47928;&#44592;&#49324; (&#45936;&#51060;&#53552;&#51032; &#47784;&#54000;&#48652;) . - 스펙이 높아도 취업이 안된다고 합니다.. . 중소·지방 기업 &quot;뽑아봤자 그만두니까&quot; . 중소기업 관계자들은 고스펙 지원자를 꺼리는 이유로 높은 퇴직률을 꼽는다. 여건이 좋은 대기업으로 이직하거나 회사를 관두는 경우가 많다는 하소연이다. 고용정보원이 지난 3일 공개한 자료에 따르면 중소기업 청년취업자 가운데 49.5%가 2년 내에 회사를 그만두는 것으로 나타났다. . 중소 IT업체 관계자는 &quot;기업 입장에서 가장 뼈아픈 게 신입사원이 그만둬서 새로 뽑는 일&quot;이라며 &quot;명문대 나온 스펙 좋은 지원자를 뽑아놔도 1년을 채우지 않고 그만두는 사원이 대부분이라 우리도 눈을 낮춰 사람을 뽑는다&quot;고 말했다. . &#44032;&#51676;&#45936;&#51060;&#53552; . - 위의 기사를 모티브로 한 데이터 . df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv&#39;) df . x underlying y . 0 -1.000000 | 0.000045 | 0.0 | . 1 -0.998999 | 0.000046 | 0.0 | . 2 -0.997999 | 0.000047 | 0.0 | . 3 -0.996998 | 0.000047 | 0.0 | . 4 -0.995998 | 0.000048 | 0.0 | . ... ... | ... | ... | . 1995 0.995998 | 0.505002 | 0.0 | . 1996 0.996998 | 0.503752 | 0.0 | . 1997 0.997999 | 0.502501 | 0.0 | . 1998 0.998999 | 0.501251 | 1.0 | . 1999 1.000000 | 0.500000 | 1.0 | . 2000 rows × 3 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; plt.plot(df.x,df.y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe719fa10&gt;] . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47196; &#51201;&#54633; . . x= torch.tensor(df.x).float().reshape(-1,1) #float(): 뒤에 거슬리는거 빼주기 y= torch.tensor(df.y).float().reshape(-1,1) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) yhat=net(x) . loss_fn = torch.nn.BCELoss() loss = loss_fn(yhat,y) # loss = -torch.mean((y)*torch.log(yhat)+(1-y)*torch.log(1-yhat)) loss . tensor(0.9367, grad_fn=&lt;BinaryCrossEntropyBackward0&gt;) . optimizr = torch.optim.Adam(net.parameters()) . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) # 학습전 . [&lt;matplotlib.lines.Line2D at 0x7f4fe8ceab10&gt;] . for epoc in range(6000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe8d7bc50&gt;] . - 이건 epoc=6억번으로 설정해도 못 맞출 것 같다 (증가하다가 감소하는 underlying을 설계하는 것이 불가능) $ to$ 모형의 표현력이 너무 낮다. . &#54644;&#44208;&#52293; . - sigmoid 넣기 전의 상태가 꺽인 그래프 이어야 한다. . sig = torch.nn.Sigmoid() . fig,ax = plt.subplots(4,2,figsize=(8,8)) u1 = torch.tensor([-6,-4,-2,0,2,4,6]) u2 = torch.tensor([6,4,2,0,-2,-4,-6]) u3 = torch.tensor([-6,-2,2,6,2,-2,-6]) u4 = torch.tensor([-6,-2,2,6,4,2,0]) ax[0,0].plot(u1,&#39;--o&#39;,color=&#39;C0&#39;);ax[0,1].plot(sig(u1),&#39;--o&#39;,color=&#39;C0&#39;) ax[1,0].plot(u2,&#39;--o&#39;,color=&#39;C1&#39;);ax[1,1].plot(sig(u2),&#39;--o&#39;,color=&#39;C1&#39;) ax[2,0].plot(u3,&#39;--o&#39;,color=&#39;C2&#39;);ax[2,1].plot(sig(u3),&#39;--o&#39;,color=&#39;C2&#39;) ax[3,0].plot(u4,&#39;--o&#39;,color=&#39;C3&#39;);ax[3,1].plot(sig(u4),&#39;--o&#39;,color=&#39;C3&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe976bfd0&gt;] . &#44618;&#51008;&#49888;&#44221;&#47581;--DNN&#51012; &#51060;&#50857;&#54620; &#54644;&#44208; . - 목표: 아래와 같은 벡터 ${ boldsymbol u}$를 만들어보자. . ${ boldsymbol u} = [u_1,u_2, dots,u_{2000}], quad u_i = begin{cases} 9x_i +4.5&amp; x_i &lt;0 -4.5x_i + 4.5&amp; x_i &gt;0 end{cases}$ . &#44733;&#51064; &#44536;&#47000;&#54532;&#47484; &#47564;&#46300;&#45716; &#48169;&#48277;1 . u = [9*xi+4.5 if xi &lt;0 else -4.5*xi+4.5 for xi in x.reshape(-1).tolist()] #tolist하면 list화 plt.plot(u,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe9689550&gt;] . &#44733;&#51064; &#44536;&#47000;&#54532;&#47484; &#47564;&#46300;&#45716; &#48169;&#48277;2 . - 전략: 선형변환 $ to$ ReLU $ to$ 선형변환 . (예비학습) ReLU 함수란? . $ReLU(x) = max(0,x)$ . relu=torch.nn.ReLU() plt.plot(x,&#39;--r&#39;) plt.plot(relu(x),&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe697ad50&gt;] . 빨간색: x, 파란색: relu(x) | . 예비학습끝 . 우리 전략 다시 확인: 선형변환1 -&gt; 렐루 -&gt; 선형변환2 . (선형변환1) . plt.plot(x);plt.plot(-x) . [&lt;matplotlib.lines.Line2D at 0x7f4fe6902710&gt;] . (렐루) . plt.plot(x,alpha=0.2);plt.plot(-x,alpha=0.5) plt.plot(relu(x),&#39;--&#39;,color=&#39;C0&#39;);plt.plot(relu(-x),&#39;--&#39;,color=&#39;C1&#39;) #out feature을 2로 잡는당-&gt;선을 두개로 . [&lt;matplotlib.lines.Line2D at 0x7f4fe6884f50&gt;] . (선형변환2) . plt.plot(x,alpha=0.2);plt.plot(-x,alpha=0.2) plt.plot(relu(x),&#39;--&#39;,color=&#39;C0&#39;,alpha=0.2);plt.plot(relu(-x),&#39;--&#39;,color=&#39;C1&#39;,alpha=0.2) plt.plot(-4.5*relu(x)-9.0*relu(-x)+4.5,&#39;--&#39;,color=&#39;C2&#39;) #하늘색 점선과 노란색 점섬을 더해보자.. . [&lt;matplotlib.lines.Line2D at 0x7f4fe67922d0&gt;] . 이제 초록색선에 sig를 취하기만 하면? . plt.plot(sig(-4.5*relu(x)-9.0*relu(-x)+4.5),&#39;--&#39;,color=&#39;C2&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe6700650&gt;] . 정리하면! . fig = plt.figure(figsize=(8, 4)) spec = fig.add_gridspec(4, 4) ax1 = fig.add_subplot(spec[:2,0]); ax1.set_title(&#39;x&#39;); ax1.plot(x,&#39;--&#39;,color=&#39;C0&#39;) ax2 = fig.add_subplot(spec[2:,0]); ax2.set_title(&#39;-x&#39;); ax2.plot(-x,&#39;--&#39;,color=&#39;C1&#39;) ax3 = fig.add_subplot(spec[:2,1]); ax3.set_title(&#39;relu(x)&#39;); ax3.plot(relu(x),&#39;--&#39;,color=&#39;C0&#39;) ax4 = fig.add_subplot(spec[2:,1]); ax4.set_title(&#39;relu(-x)&#39;); ax4.plot(relu(-x),&#39;--&#39;,color=&#39;C1&#39;) ax5 = fig.add_subplot(spec[1:3,2]); ax5.set_title(&#39;u&#39;); ax5.plot(-4.5*relu(x)-9*relu(-x)+4.5,&#39;--&#39;,color=&#39;C2&#39;) ax6 = fig.add_subplot(spec[1:3,3]); ax6.set_title(&#39;yhat&#39;); ax6.plot(sig(-4.5*relu(x)-9*relu(-x)+4.5),&#39;--&#39;,color=&#39;C2&#39;) fig.tight_layout() . 이런느낌으로 $ hat{ boldsymbol y}$을 만들면 된다. | . torch.nn.Linear()&#47484; &#51060;&#50857;&#54620; &#44733;&#51064; &#44536;&#47000;&#54532; &#44396;&#54788; . torch.manual_seed(43052) l1 = torch.nn.Linear(in_features=1,out_features=2,bias=True) a1 = torch.nn.ReLU() l2 = torch.nn.Linear(in_features=2,out_features=1,bias=True) a2 = torch.nn.Sigmoid() . net = torch.nn.Sequential(l1,a1,l2,a2) . l1.weight,l1.bias,l2.weight,l2.bias . (Parameter containing: tensor([[-0.3467], [-0.8470]], requires_grad=True), Parameter containing: tensor([0.3604, 0.9336], requires_grad=True), Parameter containing: tensor([[ 0.2880, -0.6282]], requires_grad=True), Parameter containing: tensor([0.2304], requires_grad=True)) . l1.weight.data = torch.tensor([[1.0],[-1.0]]) l1.bias.data = torch.tensor([0.0, 0.0]) l2.weight.data = torch.tensor([[ -4.5, -9.0]]) l2.bias.data= torch.tensor([4.5]) l1.weight,l1.bias,l2.weight,l2.bias . (Parameter containing: tensor([[ 1.], [-1.]], requires_grad=True), Parameter containing: tensor([0., 0.], requires_grad=True), Parameter containing: tensor([[-4.5000, -9.0000]], requires_grad=True), Parameter containing: tensor([4.5000], requires_grad=True)) . plt.plot(l1(x).data) . [&lt;matplotlib.lines.Line2D at 0x7f4fe64e0d50&gt;, &lt;matplotlib.lines.Line2D at 0x7f4fe64e0f90&gt;] . plt.plot(a1(l1(x)).data) . [&lt;matplotlib.lines.Line2D at 0x7f4fe64608d0&gt;, &lt;matplotlib.lines.Line2D at 0x7f4fe6460b10&gt;] . plt.plot(l2(a1(l1(x))).data,color=&#39;C2&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe63e3150&gt;] . plt.plot(a2(l2(a1(l1(x)))).data,color=&#39;C2&#39;) #plt.plot(net(x).data,color=&#39;C2&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe634fa50&gt;] . - 수식표현 . ${ bf X}= begin{bmatrix} x_1 dots x_n end{bmatrix}$ . | $l_1({ bf X})={ bf X}{ bf W}^{(1)} overset{bc}{+} { boldsymbol b}^{(1)}= begin{bmatrix} x_1 &amp; -x_1 x_2 &amp; -x_2 dots &amp; dots x_n &amp; -x_n end{bmatrix}$ . ${ bf W}^{(1)}= begin{bmatrix} 1 &amp; -1 end{bmatrix}$ | ${ boldsymbol b}^{(1)}= begin{bmatrix} 0 &amp; 0 end{bmatrix}$ | . | $(a_1 circ l_1)({ bf X})= text{relu} big({ bf X}{ bf W}^{(1)} overset{bc}{+}{ boldsymbol b}^{(1)} big)= begin{bmatrix} text{relu}(x_1) &amp; text{relu}(-x_1) text{relu}(x_2) &amp; text{relu}(-x_2) dots &amp; dots text{relu}(x_n) &amp; text{relu}(-x_n) end{bmatrix}$ . | $(l_2 circ a_1 circ l_1)({ bf X})= text{relu} big({ bf X}{ bf W}^{(1)} overset{bc}{+}{ boldsymbol b}^{(1)} big){ bf W}^{(2)} overset{bc}{+}b^{(2)} = begin{bmatrix} -4.5 times text{relu}(x_1) -9.0 times text{relu}(-x_1) +4.5 -4.5 times text{relu}(x_2) -9.0 times text{relu}(-x_2) + 4.5 dots -4.5 times text{relu}(x_n) -9.0 times text{relu}(-x_n)+4.5 end{bmatrix}$ . ${ bf W}^{(2)}= begin{bmatrix} -4.5 -9 end{bmatrix}$ | $b^{(2)}=4.5$ | . | $net({ bf X})=(a_2 circ l_2 circ a_1 circ l_1)({ bf X})= text{sig} Big( text{relu} big({ bf X}{ bf W}^{(1)} overset{bc}{+}{ boldsymbol b}^{(1)} big){ bf W}^{(2)} overset{bc}{+}b^{(2)} Big) = begin{bmatrix} text{sig} Big(-4.5 times text{relu}(x_1) -9.0 times text{relu}(-x_1) +4.5 Big) text{sig} Big(-4.5 times text{relu}(x_2) -9.0 times text{relu}(-x_2) + 4.5 Big) dots text{sig} Big(-4.5 times text{relu}(x_n) -9.0 times text{relu}(-x_n)+4.5 Big) end{bmatrix}$ . | - 차원만 따지자 . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,2)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,2)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcb4b0bcd50&gt;] . Step1 ~ Step4 . - 준비 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=2), #u1=l1(x), x:(n,1) --&gt; u1:(n,2) torch.nn.ReLU(), # v1=a1(u1), u1:(n,2) --&gt; v1:(n,2) torch.nn.Linear(in_features=2,out_features=1), # u2=l2(v1), v1:(n,2) --&gt; u2:(n,1) torch.nn.Sigmoid() # v2=a2(u2), u2:(n,1) --&gt; v2:(n,1) ) . loss_fn = torch.nn.BCELoss() . optimizr = torch.optim.Adam(net.parameters()) # lr은 디폴트값으로.. . - 반복 . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) plt.title(&quot;before&quot;) . Text(0.5, 1.0, &#39;before&#39;) . for epoc in range(3000): ## step1 yhat = net(x) ## step2 loss = loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;,color=&#39;C1&#39;) plt.title(&quot;after 3000 epochs&quot;) . Text(0.5, 1.0, &#39;after 3000 epochs&#39;) . for epoc in range(3000): ## step1 yhat = net(x) ## step2 loss = loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;,color=&#39;C1&#39;) plt.title(&quot;after 6000 epochs&quot;) . Text(0.5, 1.0, &#39;after 6000 epochs&#39;) . &#44618;&#51008;&#49888;&#44221;&#47581;--DNN&#51004;&#47196; &#54644;&#44208;&#44032;&#45733;&#54620; &#45796;&#50577;&#54620; &#50696;&#51228; . &#50696;&#51228;1 . - 언뜻 생각하면 방금 배운 기술은 sig를 취하기 전이 꺽은선인 형태만 가능할 듯 하다. $ to$ 그래서 이 역시 표현력이 부족할 듯 하다. $ to$ 그런데 생각보다 표현력이 풍부한 편이다. 즉 생각보다 쓸 만하다. . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex1.csv&#39;) . x = torch.tensor(df.x).float().reshape(-1,1) y = torch.tensor(df.y).float().reshape(-1,1) . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe62d7850&gt;] . 이거 시그모이드 취하기 직전은 step이 포함된 듯 $ to$ 그래서 꺽은선으로는 표현할 수 없는 구조임 $ to$ 그런데 사실 대충은 표현가능 | . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=16), # x:(n,1) --&gt; u1:(n,16) #최대 16번 꺾일 수 있음.. torch.nn.ReLU(), # u1:(n,16) --&gt; v1:(n,16) torch.nn.Linear(in_features=16,out_features=1), # v1:(n,16) --&gt; u2:(n,1) torch.nn.Sigmoid() # u2:(n,1) --&gt; v2:(n,1) ) . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,16)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,16)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ | . loss_fn = torch.nn.BCELoss() . optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(6000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) #실제로는 관츷ㄱ 못하는거 plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcb40d64ed0&gt;] . &#50696;&#51228;2 . - 사실 꺽은선의 조합으로 꽤 많은걸 표현할 수 있거든요? $ to$ 심지어 곡선도 대충 맞게 적합된다. . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex2.csv&#39;) . x = torch.tensor(df.x).float().reshape(-1,1) y = torch.tensor(df.y).float().reshape(-1,1) . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcb3ddfd210&gt;] . x=torch.tensor(df.x).float().reshape(-1,1) y=torch.tensor(df.y).float().reshape(-1,1) . (풀이1) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --&gt; u1:(n,32) torch.nn.ReLU(), # u1:(n,32) --&gt; v1:(n,32) torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --&gt; u2:(n,1) ) . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,32)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,32)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ | . loss_fn = torch.nn.MSELoss() #mseloss:마지막에 sigmoid형태가 아니니까! . optimizr = torch.optim.Adam(net.parameters()) . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcb1c74e990&gt;] . for epoc in range(6000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,lw=4) #lw:두겁게 . [&lt;matplotlib.lines.Line2D at 0x7fcb1c785150&gt;] . (풀이2) -- 풀이1보다 좀 더 잘맞음. 잘 맞는 이유? 좋은초기값 (=운) . torch.manual_seed(5) # seed값을 43052-&gt;5로바꿔주기... net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --&gt; u1:(n,32) torch.nn.ReLU(), # u1:(n,32) --&gt; v1:(n,32) torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --&gt; u2:(n,1) ) . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,32)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,32)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ | . loss_fn = torch.nn.MSELoss() . optimizr = torch.optim.Adam(net.parameters()) . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcb1c3f9e50&gt;] . for epoc in range(6000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,lw=4) . [&lt;matplotlib.lines.Line2D at 0x7fcb1bfbda50&gt;] . 풀이1에서 에폭을 많이 반복하면 풀이2의 적합선이 나올까? --&gt; 안나옴!! (local min에 빠졌다) | . &#50696;&#51228;3 . import seaborn as sns . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex3.csv&#39;) df . x1 x2 y . 0 -0.874139 | 0.210035 | 0.0 | . 1 -1.143622 | -0.835728 | 1.0 | . 2 -0.383906 | -0.027954 | 0.0 | . 3 2.131652 | 0.748879 | 1.0 | . 4 2.411805 | 0.925588 | 1.0 | . ... ... | ... | ... | . 1995 -0.002797 | -0.040410 | 0.0 | . 1996 -1.003506 | 1.182736 | 0.0 | . 1997 1.388121 | 0.079317 | 0.0 | . 1998 0.080463 | 0.816024 | 1.0 | . 1999 -0.416859 | 0.067907 | 0.0 | . 2000 rows × 3 columns . sns.scatterplot(data=df,x=&#39;x1&#39;,y=&#39;x2&#39;,hue=&#39;y&#39;,alpha=0.5,palette={0:(0.5, 0.0, 1.0),1:(1.0,0.0,0.0)}) . &lt;AxesSubplot:xlabel=&#39;x1&#39;, ylabel=&#39;x2&#39;&gt; . x1 = torch.tensor(df.x1).float().reshape(-1,1) x2 = torch.tensor(df.x2).float().reshape(-1,1) X = torch.concat([x1,x2],axis=1) y = torch.tensor(df.y).float().reshape(-1,1) . X.shape . torch.Size([2000, 2]) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=2,out_features=32),#X의 shape이 2니까 in_features=2 torch.nn.ReLU(), torch.nn.Linear(in_features=32,out_features=1), torch.nn.Sigmoid() ) . $ underset{(n,2)}{ bf X} overset{l_1}{ to} underset{(n,32)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,32)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ | . loss_fn = torch.nn.BCELoss() . optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(3000): ## 1 yhat = net(X) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . df2 = df.assign(yhat=yhat.reshape(-1).detach().tolist()) #seaborn을 그리려먼 dataframe형식으로 되어잇어야해 df2 . x1 x2 y yhat . 0 -0.874139 | 0.210035 | 0.0 | 0.345833 | . 1 -1.143622 | -0.835728 | 1.0 | 0.605130 | . 2 -0.383906 | -0.027954 | 0.0 | 0.111915 | . 3 2.131652 | 0.748879 | 1.0 | 0.918491 | . 4 2.411805 | 0.925588 | 1.0 | 0.912608 | . ... ... | ... | ... | ... | . 1995 -0.002797 | -0.040410 | 0.0 | 0.254190 | . 1996 -1.003506 | 1.182736 | 0.0 | 0.508002 | . 1997 1.388121 | 0.079317 | 0.0 | 0.410099 | . 1998 0.080463 | 0.816024 | 1.0 | 0.262315 | . 1999 -0.416859 | 0.067907 | 0.0 | 0.107903 | . 2000 rows × 4 columns . sns.scatterplot(data=df2,x=&#39;x1&#39;,y=&#39;x2&#39;,hue=&#39;yhat&#39;,alpha=0.5,palette=&#39;rainbow&#39;) . &lt;AxesSubplot:xlabel=&#39;x1&#39;, ylabel=&#39;x2&#39;&gt; . - 결과시각화 . fig, ax = plt.subplots(1,2,figsize=(8,4)) sns.scatterplot(data=df,x=&#39;x1&#39;,y=&#39;x2&#39;,hue=&#39;y&#39;,alpha=0.5,palette={0:(0.5, 0.0, 1.0),1:(1.0,0.0,0.0)},ax=ax[0]) sns.scatterplot(data=df2,x=&#39;x1&#39;,y=&#39;x2&#39;,hue=&#39;yhat&#39;,alpha=0.5,palette=&#39;rainbow&#39;,ax=ax[1]) . &lt;AxesSubplot:xlabel=&#39;x1&#39;, ylabel=&#39;x2&#39;&gt; . - 교훈: underlying이 엄청 이상해보여도 생각보다 잘 맞춤 .",
            "url": "https://boram-coco.github.io/coco1/2022/09/28/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9405%EC%9D%BC.html",
            "relUrl": "/2022/09/28/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9405%EC%9D%BC.html",
            "date": " • Sep 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "기계학습특강",
            "content": "# 우리의 2차 목표: 그 모형에 &quot;새로운&quot; 자료를 전달하여 이미지를 분류할 것이다. (즉 클래시파이어를 쓴다는 소리) . from fastai.vision.all import * . path=untar_data(URLs.PETS)/&#39;images&#39; . . 100.00% [811712512/811706944 01:16&lt;00:00] path . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;) . PILImage.create(&#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39;) . _lst = &#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39;,&#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_10.jpg&#39; . _lst[0] . &#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39; . faaaa = get_image_files(path) #교수님은 filenames로 설정함 . faaaa[0] . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/leonberger_137.jpg&#39;) . PILImage.create(&#39;/root/.fastai/data/oxford-iiit-pet/images/newfoundland_28.jpg&#39;) . PILImage.create(faaaa[0]) . print(faaaa[1]) PILImage.create(faaaa[1]) . /root/.fastai/data/oxford-iiit-pet/images/Birman_139.jpg . print(faaaa[3]) PILImage.create(faaaa[3]) . /root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_148.jpg . print(faaaa[6]) PILImage.create(faaaa[6]) . /root/.fastai/data/oxford-iiit-pet/images/Siamese_79.jpg . &#39;A&#39;.isupper() . True . &#39;abdjlkfwe.jpg&#39;[0] . &#39;a&#39; . def f(fname): if fname[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . dls = ImageDataLoaders.from_name_func(path,faaaa,f,item_tfms=Resize(224)) . dls.show_batch(max_n=25) . # 우리의 2차 목표: 그 모형에 &quot;새로운&quot; 자료를 전달하여 이미지를 분류할 것이다. (즉 클래시파이어를 쓴다는 소리) ## 오브젝트.. 오브젝트에는 동사와 명사 가 있어요 ### 명사 # (1) 데이터 # (2) 채용한 모형의 이론 # (3) 평가기준 matric ### 동사 # (1) 학습 # (2) 판단 . ysj = cnn_learner(dls,resnet34,metrics=error_rate) ##저항률 확인(잘 파악하는지 확인하기 위해서 metrics=error_rate 이용) ysj.fine_tune(1) ## 3을 쓰면 1보다는 많이 한다는 뜻 ## 학습하다..동사,, . /usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code warn(&#34;`cnn_learner` has been renamed to `vision_learner` -- please update your code&#34;) /usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter &#39;pretrained&#39; is deprecated since 0.13 and will be removed in 0.15, please use &#39;weights&#39; instead. f&#34;The parameter &#39;{pretrained_param}&#39; is deprecated since 0.13 and will be removed in 0.15, &#34; /usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg) Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth . epoch train_loss valid_loss error_rate time . 0 | 0.147350 | 0.014042 | 0.004060 | 00:56 | . epoch train_loss valid_loss error_rate time . 0 | 0.053051 | 0.012090 | 0.004736 | 00:52 | . ?cnn_learner . PILImage.create(faaaa[1]) . ysj.predict(PILImage.create(faaaa[1])) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 3.0773e-10])) . ysj.predict(PILImage.create(faaaa[6])) . &#46041;&#49324;&#51060;&#44256;.. &#46244;&#50640; &#51216;&#52237;&#50632;&#51004;&#45768;&#44620; &#54632;&#49688;&#45796; &#49373;&#44033;&#54616;&#44592; . &#51077;&#47141;&#51012; &#51060;&#48120;&#51648; &#51088;&#52404;&#47196; &#45347;&#50632;&#45716;&#45936;, &#51060;&#48120;&#51648;&#44032; &#51200;&#51109;&#46108; path&#47564; &#45347;&#50612;&#46020; &#46104;&#51648;&#50506;&#51012;&#44620;? . ysj.predict(faaaa[6]) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 2.2963e-10])) . ysj.show_results() . # 체크를 하는 object를 만들어야함 checker = Interpretation.from_learner(ysj) . checker.plot_top_losses(16) # 첫번째 사진에서 5.59는 로스이고 1퍼의 확률로 강아지라고 생각함 # 로스는 몇퍼의 확률로 잘못생각했느냐에 따라서 달라질 수 있음 # 맞추는 걸 넘어서 확실해야 로스가 적다. (확신의여부) # 오버피팅 아냐..? 과대적합..? 자기들이 이미 다학습된 내용 가지고 보여주는거아냐? 생각-&gt;새로운 이미지 부여 . img=PILImage.create(requests.get(&#39;https://dimg.donga.com/ugc/CDB/SHINDONGA/Article/5e/0d/9f/01/5e0d9f011a9ad2738de6.jpg&#39;).content) ysj.predict(img) . (&#39;dog&#39;, TensorBase(1), TensorBase([2.8106e-06, 1.0000e+00])) . img=PILImage.create(requests.get(&#39;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcnSq1X%2Fbtq4o9AdWTH%2FHTm9TZG4AszSwLPFlVfGW0%2Fimg.jpg&#39;).content) ysj.predict(img) . (&#39;dog&#39;, TensorBase(1), TensorBase([1.0909e-06, 1.0000e+00])) . img=PILImage.create(requests.get(&#39;https://image.edaily.co.kr/images/photo/files/NP/S/2022/04/PS22042501396.jpg&#39;).content) ysj.predict(img) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 2.6542e-12])) . img=PILImage.create(requests.get(&#39;https://blog.kakaocdn.net/dn/zfQQi/btrydI0vGzm/3YY3KrPEwKN558e27H6t0k/img.jpg&#39;).content) ysj.predict(img) . (&#39;cat&#39;, TensorBase(0), TensorBase([0.9805, 0.0195])) . PILImage.create(&#39;/강아지사진1.jpg&#39;) .",
            "url": "https://boram-coco.github.io/coco1/2022/09/07/ML.html",
            "relUrl": "/2022/09/07/ML.html",
            "date": " • Sep 7, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://boram-coco.github.io/coco1/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://boram-coco.github.io/coco1/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "about me에 대한 관련된 내용 작성 . 프로젝트(수업들으면서 했던 프로젝트를 쓰든지,,) . 신기하다 ㅇ와 . 시계열분석을 활용한 삼ㅈ성전자 주식예측(2022-05) . 이런식으로… . 프로그래밍 스킬 ( R 파이썬 .. ) .",
          "url": "https://boram-coco.github.io/coco1/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://boram-coco.github.io/coco1/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}