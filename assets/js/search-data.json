{
  
    
        "post0": {
            "title": "(8주차) 10월26일--(1)",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import torch import torchvision import numpy as np from fastai.vision.all import * . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;); . CNN &#45796;&#51473;&#53364;&#47000;&#49828; &#48516;&#47448; . &#44208;&#47200; (&#44536;&#45285; &#50808;&#50864;&#49464;&#50836;) . - 2개의 class를 구분하는 문제가 아니라 $k$개의 class를 구분해야 한다면? . 일반적인 개념 . 손실함수: BCE loss $ to$ Cross Entropy loss | 마지막층의 선형변환: torch.nn.Linear(?,1) $ to$ torch.nn.Linear(?,k) | 마지막층의 활성화: sig $ to$ softmax | . 파이토치 한정 . y의형태: (n,) vector + int형 // (n,k) one-hot encoded vector + float형 | 손실함수: torch.nn.BCEWithLogitsLoss, $ to$ torch.nn.CrossEntropyLoss | 마지막층의 선형변환: torch.nn.Linear(?,1) $ to$ torch.nn.Linear(?,k) | 마지막층의 활성화: None $ to$ None (손실함수에 이미 마지막층의 활성화가 포함) | . . &#49892;&#49845;: 3&#44060;&#51032; &#53364;&#47000;&#49828;&#47484; &#44396;&#48516; . path = untar_data(URLs.MNIST) . . 100.03% [15687680/15683414 00:00&lt;00:00] training set . X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;training/0&#39;).ls()]) X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;training/1&#39;).ls()]) X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;training/2&#39;).ls()]) X = torch.concat([X0,X1,X2])/255 y = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1) . y . tensor([0, 0, 0, ..., 2, 2, 2]) . test set . X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;testing/0&#39;).ls()]) X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;testing/1&#39;).ls()]) X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;testing/2&#39;).ls()]) XX = torch.concat([X0,X1,X2])/255 yy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1) . (1) dls . len(X) . 18623 . len(XX) . 3147 . ds1 = torch.utils.data.TensorDataset(X,y) ds2 = torch.utils.data.TensorDataset(XX,yy) dl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번 iter dl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # dls = DataLoaders(dl1,dl2) . (2) lrnr . net1 = torch.nn.Sequential( torch.nn.Conv2d(1,16,(5,5)), torch.nn.ReLU(), torch.nn.MaxPool2d((2,2)), torch.nn.Flatten() ) . net1(X).shape . torch.Size([18623, 2304]) . net = torch.nn.Sequential( net1, torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 ) loss_fn = torch.nn.CrossEntropyLoss() . lrnr = Learner(dls,net,loss_fn) . (3) 학습 . lrnr.fit(10) . epoch train_loss valid_loss time . 0 | 0.155762 | 0.045966 | 00:00 | . 1 | 0.087853 | 0.093116 | 00:00 | . 2 | 0.089215 | 0.033146 | 00:00 | . 3 | 0.075707 | 0.039687 | 00:00 | . 4 | 0.063615 | 0.031188 | 00:00 | . 5 | 0.055850 | 0.029167 | 00:00 | . 6 | 0.050883 | 0.028610 | 00:00 | . 7 | 0.047144 | 0.028127 | 00:00 | . 8 | 0.044129 | 0.027425 | 00:00 | . 9 | 0.041849 | 0.026887 | 00:00 | . (4) 예측 . lrnr.model.to(&quot;cpu&quot;) . Sequential( (0): Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False) (3): Flatten(start_dim=1, end_dim=-1) ) (1): Linear(in_features=2304, out_features=3, bias=True) ) . pd.DataFrame(lrnr.model(XX)).assign(y=yy) . 0 1 2 y . 0 2.025615 | -17.453382 | -2.335629 | 0 | . 1 2.531184 | -9.890347 | -3.949971 | 0 | . 2 1.587324 | -13.035635 | -3.257438 | 0 | . 3 2.749664 | -9.918694 | -6.867003 | 0 | . 4 2.155442 | -15.885134 | -3.019295 | 0 | . ... ... | ... | ... | ... | . 3142 -9.030737 | -8.673495 | 1.204605 | 2 | . 3143 -3.569304 | -10.748278 | -0.645055 | 2 | . 3144 -3.627095 | -12.919289 | 1.046083 | 2 | . 3145 -5.151307 | -9.524368 | 0.068864 | 2 | . 3146 -8.014889 | -14.518770 | 2.732226 | 2 | . 3147 rows × 4 columns . pd.DataFrame(lrnr.model(XX)).assign(y=yy).query(&#39;y==0&#39;) . 0 1 2 y . 0 2.025615 | -17.453382 | -2.335629 | 0 | . 1 2.531184 | -9.890347 | -3.949971 | 0 | . 2 1.587324 | -13.035635 | -3.257438 | 0 | . 3 2.749664 | -9.918694 | -6.867003 | 0 | . 4 2.155442 | -15.885134 | -3.019295 | 0 | . ... ... | ... | ... | ... | . 975 4.450414 | -19.329332 | -5.451057 | 0 | . 976 2.597144 | -21.202513 | -2.372313 | 0 | . 977 3.089397 | -15.486772 | -3.171502 | 0 | . 978 2.591355 | -17.475590 | -3.039083 | 0 | . 979 4.575789 | -21.636261 | -5.077421 | 0 | . 980 rows × 4 columns . 대체적으로 첫번째 칼럼의 숫자들이 다른칼럼보다 크다. | . pd.DataFrame(lrnr.model(XX)).assign(y=yy).query(&#39;y==1&#39;) . 0 1 2 y . 980 -7.629252 | 2.701625 | -3.049654 | 1 | . 981 -6.666027 | 2.027717 | -4.440878 | 1 | . 982 -7.120140 | 3.354897 | -3.937105 | 1 | . 983 -7.139010 | 2.058928 | -2.954304 | 1 | . 984 -6.999043 | 3.258377 | -3.432081 | 1 | . ... ... | ... | ... | ... | . 2110 -7.406105 | 4.032059 | -3.826451 | 1 | . 2111 -6.172441 | 3.187883 | -3.490717 | 1 | . 2112 -7.035848 | 3.202726 | -3.547020 | 1 | . 2113 -7.149322 | 1.741414 | -1.765136 | 1 | . 2114 -5.775263 | 3.041395 | -2.980052 | 1 | . 1135 rows × 4 columns . 대체적으로 두번째 칼럼의 숫자들이 다른칼럼보다 크다. | . pd.DataFrame(lrnr.model(XX)).assign(y=yy).query(&#39;y==2&#39;) . 0 1 2 y . 2115 -7.452892 | -7.137640 | 0.605079 | 2 | . 2116 -5.262731 | -7.798437 | -1.708323 | 2 | . 2117 -9.056837 | -9.649239 | 1.557607 | 2 | . 2118 -6.614496 | -10.291727 | 1.293422 | 2 | . 2119 -3.120885 | -10.670292 | -0.660188 | 2 | . ... ... | ... | ... | ... | . 3142 -9.030737 | -8.673495 | 1.204605 | 2 | . 3143 -3.569304 | -10.748278 | -0.645055 | 2 | . 3144 -3.627095 | -12.919289 | 1.046083 | 2 | . 3145 -5.151307 | -9.524368 | 0.068864 | 2 | . 3146 -8.014889 | -14.518770 | 2.732226 | 2 | . 1032 rows × 4 columns . 대체적으로 세번째 칼럼의 숫자들이 다른칼럼보다 크다. | . - 예측하는방법? . 칼럼0의 숫자가 크다 -&gt; y=0일 확률이 큼 | 칼럼1의 숫자가 크다 -&gt; y=1일 확률이 큼 | 칼럼2의 숫자가 크다 -&gt; y=2일 확률이 큼 | . &#44277;&#48512;: Softmax . - 눈치: softmax를 쓰기 직전의 숫자들은 (n,k)꼴로 되어있음. 각 observation 마다 k개의 숫자가 있는데, 그중에서 유난히 큰 하나의 숫자가 있음. . - torch.nn.Softmax() 손계산 . (예시1) -- 잘못계산 . sftmax = torch.nn.Softmax(dim=0) . _netout = torch.tensor([[-2.0,-2.0,0.0], [3.14,3.14,3.14], [0.0,0.0,2.0], [2.0,2.0,4.0], [0.0,0.0,0.0]]) #위 식은 예시.. _netout . tensor([[-2.0000, -2.0000, 0.0000], [ 3.1400, 3.1400, 3.1400], [ 0.0000, 0.0000, 2.0000], [ 2.0000, 2.0000, 4.0000], [ 0.0000, 0.0000, 0.0000]]) . sftmax(_netout) #세로로 더하면 1이 되네? 이상하게 됨.. dimension방향이 잘못되어있음 . tensor([[0.0041, 0.0041, 0.0115], [0.7081, 0.7081, 0.2653], [0.0306, 0.0306, 0.0848], [0.2265, 0.2265, 0.6269], [0.0306, 0.0306, 0.0115]]) . (예시2) -- 이게 맞게 계산되는 것임 . sftmax = torch.nn.Softmax(dim=1) . _netout . tensor([[-2.0000, -2.0000, 0.0000], [ 3.1400, 3.1400, 3.1400], [ 0.0000, 0.0000, 2.0000], [ 2.0000, 2.0000, 4.0000], [ 0.0000, 0.0000, 0.0000]]) . sftmax(_netout) . tensor([[0.1065, 0.1065, 0.7870], [0.3333, 0.3333, 0.3333], [0.1065, 0.1065, 0.7870], [0.1065, 0.1065, 0.7870], [0.3333, 0.3333, 0.3333]]) . (예시3) -- 차원을 명시안하면 맞게 계산해주고 경고 줌 . sftmax = torch.nn.Softmax() . _netout . tensor([[-2.0000, -2.0000, 0.0000], [ 3.1400, 3.1400, 3.1400], [ 0.0000, 0.0000, 2.0000], [ 2.0000, 2.0000, 4.0000], [ 0.0000, 0.0000, 0.0000]]) . sftmax(_netout) . /home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. &#34;&#34;&#34;Entry point for launching an IPython kernel. . tensor([[0.1065, 0.1065, 0.7870], [0.3333, 0.3333, 0.3333], [0.1065, 0.1065, 0.7870], [0.1065, 0.1065, 0.7870], [0.3333, 0.3333, 0.3333]]) . (예시4) -- 진짜 손계산 . _netout . tensor([[-2.0000, -2.0000, 0.0000], [ 3.1400, 3.1400, 3.1400], [ 0.0000, 0.0000, 2.0000], [ 2.0000, 2.0000, 4.0000], [ 0.0000, 0.0000, 0.0000]]) . torch.exp(_netout) . tensor([[ 0.1353, 0.1353, 1.0000], [23.1039, 23.1039, 23.1039], [ 1.0000, 1.0000, 7.3891], [ 7.3891, 7.3891, 54.5981], [ 1.0000, 1.0000, 1.0000]]) . 0.1353/(0.1353 + 0.1353 + 1.0000), 0.1353/(0.1353 + 0.1353 + 1.0000), 1.0000/(0.1353 + 0.1353 + 1.0000) # 첫 obs . (0.10648512513773022, 0.10648512513773022, 0.7870297497245397) . np.exp(_netout[1])/np.exp(_netout[1]).sum() # 두번째 obs . tensor([0.3333, 0.3333, 0.3333]) . np.apply_along_axis(lambda x: np.exp(x) / np.exp(x).sum(),1,_netout) # 1은 축방향 . array([[0.10650698, 0.10650698, 0.78698605], [0.33333334, 0.33333334, 0.33333334], [0.10650699, 0.10650699, 0.78698605], [0.10650698, 0.10650698, 0.78698605], [0.33333334, 0.33333334, 0.33333334]], dtype=float32) . &#44277;&#48512;: CrossEntropyLoss . # torch.nn.CrossEntropyLoss() &#49552;&#44228;&#49328;: one-hot version . loss_fn = torch.nn.CrossEntropyLoss() . _netout . tensor([[-2.0000, -2.0000, 0.0000], [ 3.1400, 3.1400, 3.1400], [ 0.0000, 0.0000, 2.0000], [ 2.0000, 2.0000, 4.0000], [ 0.0000, 0.0000, 0.0000]]) . _y_onehot = torch.tensor([[0,0,1], [0,1,0], [0,0,1], [0,0,1], [1,0,0]])*1.0 #y를 float 형태로 만들어야 하므로 *1.0을 한다. _y_onehot . tensor([[0., 0., 1.], [0., 1., 0.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.]]) . sftmax = torch.nn.Softmax(dim=1) sftmax(_netout), _y_onehot . (tensor([[0.1065, 0.1065, 0.7870], [0.3333, 0.3333, 0.3333], [0.1065, 0.1065, 0.7870], [0.1065, 0.1065, 0.7870], [0.3333, 0.3333, 0.3333]]), tensor([[0., 0., 1.], [0., 1., 0.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.]])) . - 계산결과 . loss_fn(_netout,_y_onehot) . tensor(0.5832) . - torch.sum(torch.log(sftmax(_netout)) * _y_onehot)/5 . tensor(0.5832) . - 계산하는 방법도 중요한데 torch.nn.CrossEntropyLoss() 에는 softmax 활성화함수가 이미 포함되어 있다는 것을 확인하는 것이 더 중요함. . - 따라서 torch.nn.CrossEntropyLoss() 는 사실 torch.nn.CEWithSoftmaxLoss() 정도로 바꾸는 것이 더 말이 되는 것 같다. . # torch.nn.CrossEntropyLoss() &#49552;&#44228;&#49328;: lenght $n$ vertor version . _netout . tensor([[-2.0000, -2.0000, 0.0000], [ 3.1400, 3.1400, 3.1400], [ 0.0000, 0.0000, 2.0000], [ 2.0000, 2.0000, 4.0000], [ 0.0000, 0.0000, 0.0000]]) . _y = torch.tensor([2,1,2,2,0]) #이해안감! #벡터로 만들고.. int만들려고..??? float가 되면 계산이 안된다. int형만 가능해.. . loss_fn(_netout,_y) . tensor(0.5832) . &#49892;&#49845;: $k=2$&#47196; &#46160;&#47732; &#51060;&#51652;&#48516;&#47448;&#46020; &#44032;&#45733; . - download data . path = untar_data(URLs.MNIST) . training . X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;training/0&#39;).ls()]) X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;training/1&#39;).ls()]) X = torch.concat([X0,X1])/255 y = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1) . y.shape . # return [1,0] # else # return [0,1] . y_onehot = torch.nn.functional.one_hot(y).float() #y_onehot = torch.tensor(list(map(lambda x: [1,0] if x==0 else [0,1],y))).float() . test . X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;testing/0&#39;).ls()]) X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;testing/1&#39;).ls()]) XX = torch.concat([X0,X1])/255 yy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1) . yy.shape . yy_onehot = torch.nn.functional.one_hot(yy).float() #yy_onehot = torch.tensor(list(map(lambda x: [1,0] if x==0 else [0,1],yy))).float() . (1) dls . ds1 = torch.utils.data.TensorDataset(X,y_onehot) ds2 = torch.utils.data.TensorDataset(XX,yy_onehot) dl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번 iter dl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # dls = DataLoaders(dl1,dl2) . (2) lrnr . net = torch.nn.Sequential( torch.nn.Conv2d(1,16,(5,5)), torch.nn.ReLU(), torch.nn.MaxPool2d((2,2)), torch.nn.Flatten(), torch.nn.Linear(2304,2) #torch.nn.Softmax() ) loss_fn = torch.nn.CrossEntropyLoss() lrnr = Learner(dls,net,loss_fn) . (3) 학습 . lrnr.fit(10) . epoch train_loss valid_loss time . 0 | 0.018623 | 0.012110 | 00:00 | . 1 | 0.017453 | 0.010544 | 00:00 | . 2 | 0.016391 | 0.009324 | 00:00 | . 3 | 0.015412 | 0.008343 | 00:00 | . 4 | 0.014506 | 0.007542 | 00:00 | . 5 | 0.013664 | 0.006874 | 00:00 | . 6 | 0.012881 | 0.006318 | 00:00 | . 7 | 0.012152 | 0.005849 | 00:00 | . 8 | 0.011473 | 0.005448 | 00:00 | . 9 | 0.010841 | 0.005103 | 00:00 | . (4) 예측 및 시각화 . lrnr.model.to(&quot;cpu&quot;) . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False) (3): Flatten(start_dim=1, end_dim=-1) (4): Linear(in_features=2304, out_features=2, bias=True) ) . sftmax = torch.nn.Softmax(dim=1) sig = torch.nn.Sigmoid() fig,ax = plt.subplots(1,2,figsize=(8,4)) ax[0].plot(net(X).diff(axis=1).data,&#39;,&#39;,color=&quot;C1&quot;) ax[1].plot(y) ax[1].plot(sftmax(net(X))[:,1].data,&#39;,&#39;) #ax[1].plot(sig(net(X).diff(axis=1)).data,&#39;,&#39;) fig.suptitle(&quot;Training Set&quot;,size=15) . Text(0.5, 0.98, &#39;Training Set&#39;) . fig,ax = plt.subplots(1,2,figsize=(8,4)) ax[0].plot(net(XX).diff(axis=1).data,&#39;,&#39;,color=&quot;C1&quot;) ax[1].plot(yy) ax[1].plot(sftmax(net(XX))[:,1].data,&#39;,&#39;) #ax[1].plot(sig(net(XX).diff(axis=1)).data,&#39;,&#39;) fig.suptitle(&quot;Test Set&quot;,size=15) . Text(0.5, 0.98, &#39;Test Set&#39;) . - note: softmax(u1,u2)=[sig(u1-u2), sig(u2-u1)]=[1-sig(u2-u1),sig(u2-u1)] . &#44277;&#48512;: &#51060;&#51652;&#48516;&#47448;&#50640;&#49436; &#49548;&#54532;&#53944;&#47589;&#49828; vs &#49884;&#44536;&#47784;&#51060;&#46300; . - 이진분류문제 = &quot;y=0 or y=1&quot; 을 맞추는 문제 = 성공과 실패를 맞추는 문제 = 성공확률과 실패확률을 추정하는 문제 . - softmax, sigmoid . softmax: (실패확률, 성공확률) 꼴로 결과가 나옴 // softmax는 실패확률과 성공확률을 둘다 추정한다. | sigmoid: (성공확률) 꼴로 결과가 나옴 // sigmoid는 성공확률만 추정한다. | . - 그런데 &quot;실패확률=1-성공확률&quot; 이므로 사실상 둘은 같은걸 추정하는 셈이다. (성공확률만 추정하면 실패확률은 저절로 추정되니까) . - 아래는 사실상 같은 모형이다. . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;?&quot; &quot;??&quot; &quot;..&quot; &quot;???&quot; label = &quot;Layer ?&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;?&quot; -&gt; &quot;node1&quot; &quot;??&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;???&quot; -&gt; &quot;node1&quot; &quot;?&quot; -&gt; &quot;node2&quot; &quot;??&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;???&quot; -&gt; &quot;node2&quot; &quot;?&quot; -&gt; &quot;...&quot; &quot;??&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;???&quot; -&gt; &quot;...&quot; &quot;?&quot; -&gt; &quot;node2304&quot; &quot;??&quot; -&gt; &quot;node2304&quot; &quot;..&quot; -&gt; &quot;node2304&quot; &quot;???&quot; -&gt; &quot;node2304&quot; label = &quot;Layer: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y1&quot; &quot;node2&quot; -&gt; &quot;y1&quot; &quot;...&quot; -&gt; &quot;y1&quot; &quot;node2304&quot; -&gt; &quot;y1&quot; &quot;node1&quot; -&gt; &quot;y2&quot; &quot;node2&quot; -&gt; &quot;y2&quot; &quot;...&quot; -&gt; &quot;y2&quot; &quot;node2304&quot; -&gt; &quot;y2&quot; label = &quot;Layer: Softmax&quot; } &#39;&#39;&#39;) . . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;?&quot; &quot;??&quot; &quot;..&quot; &quot;???&quot; label = &quot;Layer ?&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;?&quot; -&gt; &quot;node1&quot; &quot;??&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;???&quot; -&gt; &quot;node1&quot; &quot;?&quot; -&gt; &quot;node2&quot; &quot;??&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;???&quot; -&gt; &quot;node2&quot; &quot;?&quot; -&gt; &quot;...&quot; &quot;??&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;???&quot; -&gt; &quot;...&quot; &quot;?&quot; -&gt; &quot;node2304&quot; &quot;??&quot; -&gt; &quot;node2304&quot; &quot;..&quot; -&gt; &quot;node2304&quot; &quot;???&quot; -&gt; &quot;node2304&quot; label = &quot;Layer: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node2304&quot; -&gt; &quot;y&quot; label = &quot;Layer: Sigmoid&quot; } &#39;&#39;&#39;) . . - 둘은 사실상 같은 효과를 주는 모형인데 학습할 파라메터는 sigmoid의 경우가 더 적다. $ to$ sigmoid를 사용하는 모형이 비용은 싸고 효과는 동일하다는 말 $ to$ 이진분류 한정해서는 softmax를 쓰지말고 sigmoid를 써야함. . softmax가 갑자기 너무 안좋아보이는데 sigmoid는 k개의 클래스로 확장이 불가능한 반면 softmax는 확장이 용이하다는 장점이 있음 | . &#49548;&#54532;&#53944;&#47589;&#49828; vs &#49884;&#44536;&#47784;&#51060;&#46300; &#51221;&#47532; . - 결론 . 소프트맥스는 시그모이드의 확장이다. | 클래스의 수가 2개일 경우에는 (Sigmoid, BCEloss) 조합을 사용해야 하고 클래스의 수가 2개보다 클 경우에는 (Softmax, CrossEntropyLoss) 를 사용해야 한다. | - 그런데 사실.. 클래스의 수가 2개일 경우일때 (Softmax, CrossEntropyLoss)를 사용해도 그렇게 큰일나는것은 아니다. (흑백이미지를 칼라잉크로 출력하는 느낌) . 참고 . $y$ 분포가정 마지막층의 활성화함수 손실함수 . 3.45, 4.43, ... (연속형) | 정규분포 | None (or Identity) | MSE | . 0 or 1 | 이항분포 with $n=1$ (=베르누이) | Sigmoid | BCE | . [0,0,1], [0,1,0], [1,0,0] | 다항분포 with $n=1$ | Softmax | Cross Entropy | . fastai metric &#49324;&#50857; . &#45936;&#51060;&#53552;&#51456;&#48708; . - download data . path = untar_data(URLs.MNIST) . - training set . X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;training/0&#39;).ls()]) X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;training/1&#39;).ls()]) X = torch.concat([X0,X1])/255 y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1) . - test set . X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;testing/0&#39;).ls()]) X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/&#39;testing/1&#39;).ls()]) XX = torch.concat([X0,X1])/255 yy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1) . X.shape,XX.shape,y.shape,yy.shape . (torch.Size([12665, 1, 28, 28]), torch.Size([2115, 1, 28, 28]), torch.Size([12665, 1]), torch.Size([2115, 1])) . &#49324;&#50857;&#51088;&#51221;&#51032; &#47700;&#53944;&#47533;&#51060;&#50857; . (1) dls 만들기 . ds1 = torch.utils.data.TensorDataset(X,y) ds2 = torch.utils.data.TensorDataset(XX,yy) dl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) dl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) dls = DataLoaders(dl1,dl2) . (2) lrnr 생성 . net = torch.nn.Sequential( torch.nn.Conv2d(1,16,(5,5)), torch.nn.ReLU(), torch.nn.MaxPool2d((2,2)), torch.nn.Flatten(), torch.nn.Linear(2304,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() . def acc(yhat,y) : return ((yhat&gt;0.5)==y).float().mean() . def err(yhat,y): return 1-((yhat&gt;0.5)==y).float().mean() . lrnr = Learner(dls,net,loss_fn,metrics=[acc,err]) . (3) 학습 . lrnr.fit(10) . epoch train_loss valid_loss acc err time . 0 | 0.930635 | 0.633062 | 0.463357 | 0.536643 | 00:00 | . 1 | 0.700525 | 0.435254 | 0.989125 | 0.010875 | 00:00 | . 2 | 0.562099 | 0.293302 | 0.992435 | 0.007565 | 00:00 | . 3 | 0.453646 | 0.169834 | 0.992908 | 0.007092 | 00:00 | . 4 | 0.360984 | 0.098153 | 0.994326 | 0.005674 | 00:00 | . 5 | 0.286862 | 0.062732 | 0.993853 | 0.006147 | 00:00 | . 6 | 0.229778 | 0.044261 | 0.994799 | 0.005201 | 00:00 | . 7 | 0.185838 | 0.032975 | 0.995745 | 0.004255 | 00:00 | . 8 | 0.151484 | 0.025062 | 0.996217 | 0.003783 | 00:00 | . 9 | 0.124157 | 0.019350 | 0.996690 | 0.003310 | 00:00 | . (4) 예측 . 생략 | . fastai&#51648;&#50896; &#47700;&#53944;&#47533;&#51060;&#50857;-- &#51096;&#47803;&#46108;&#49324;&#50857; . (1) dls 만들기 . ds1 = torch.utils.data.TensorDataset(X,y) ds2 = torch.utils.data.TensorDataset(XX,yy) dl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) dl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) dls = DataLoaders(dl1,dl2) . (2) lrnr 생성 . net = torch.nn.Sequential( torch.nn.Conv2d(1,16,(5,5)), torch.nn.ReLU(), torch.nn.MaxPool2d((2,2)), torch.nn.Flatten(), torch.nn.Linear(2304,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() lrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate]) . accuracy?? . Signature: accuracy(inp, targ, axis=-1) Source: def accuracy(inp, targ, axis=-1): &#34;Compute accuracy with `targ` when `pred` is bs * n_classes&#34; pred,targ = flatten_check(inp.argmax(dim=axis), targ) return (pred == targ).float().mean() File: ~/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/metrics.py Type: function . error_rate?? . Signature: error_rate(inp, targ, axis=-1) Source: def error_rate(inp, targ, axis=-1): &#34;1 - `accuracy`&#34; return 1 - accuracy(inp, targ, axis=axis) File: ~/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/metrics.py Type: function . (3) 학습 . lrnr.fit(10) . epoch train_loss valid_loss accuracy error_rate time . 0 | 0.958819 | 0.638672 | 0.463357 | 0.536643 | 00:00 | . 1 | 0.698769 | 0.425380 | 0.463357 | 0.536643 | 00:00 | . 2 | 0.556408 | 0.278437 | 0.463357 | 0.536643 | 00:00 | . 3 | 0.447104 | 0.153257 | 0.463357 | 0.536643 | 00:00 | . 4 | 0.352915 | 0.088516 | 0.463357 | 0.536643 | 00:00 | . 5 | 0.278620 | 0.056958 | 0.463357 | 0.536643 | 00:00 | . 6 | 0.221951 | 0.040489 | 0.463357 | 0.536643 | 00:00 | . 7 | 0.178791 | 0.030974 | 0.463357 | 0.536643 | 00:00 | . 8 | 0.145480 | 0.024886 | 0.463357 | 0.536643 | 00:00 | . 9 | 0.119392 | 0.020659 | 0.463357 | 0.536643 | 00:00 | . 이상하다..? | . (4) 예측 . lrnr.model.to(&quot;cpu&quot;) . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False) (3): Flatten(start_dim=1, end_dim=-1) (4): Linear(in_features=2304, out_features=1, bias=True) (5): Sigmoid() ) . plt.plot(yy) plt.plot(lrnr.model(XX).data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd1ecea95d0&gt;] . 맞추는건 잘 맞추는데? | . fastai&#51648;&#50896; &#47700;&#53944;&#47533;&#51060;&#50857;-- &#50732;&#48148;&#47480; &#49324;&#50857;(1) . - 가정 . X의 형태는 (n,채널,픽셀,픽셀)로 가정한다. | y의 형태는 (n,) 벡터이다. 즉 $n times 1$ 이 아니라 그냥 길이가 $n$인 벡터로 가정한다. | y의 각 원소는 0,1,2,3,... 와 같이 카테고리를 의미하는 숫자이어야 하며 이 숫자는 int형으로 저장되어야 한다. | loss function은 CrossEntropyLoss()를 쓴다고 가정한다. (따라서 네트워크의 최종레이어는 torch.nn.Linear(?,클래스의수) 꼴이 되어야 한다.) | . (1) dls 만들기 . y.to(torch.int64).reshape(-1),yy.to(torch.int64).reshape(-1) . (tensor([0, 0, 0, ..., 1, 1, 1]), tensor([0, 0, 0, ..., 1, 1, 1])) . ds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1)) ds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1)) dl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) dl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) dls = DataLoaders(dl1,dl2) . (2) lrnr 생성 . net = torch.nn.Sequential( torch.nn.Conv2d(1,16,(5,5)), torch.nn.ReLU(), torch.nn.MaxPool2d((2,2)), torch.nn.Flatten(), torch.nn.Linear(2304,2), ) loss_fn = torch.nn.CrossEntropyLoss() lrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate]) . (3) 학습 . lrnr.fit(10) . epoch train_loss valid_loss accuracy error_rate time . 0 | 1.083689 | 0.592268 | 0.463357 | 0.536643 | 00:00 | . 1 | 0.674220 | 0.340684 | 0.963593 | 0.036407 | 00:00 | . 2 | 0.510727 | 0.152380 | 0.991489 | 0.008511 | 00:00 | . 3 | 0.380071 | 0.069530 | 0.997163 | 0.002837 | 00:00 | . 4 | 0.284898 | 0.038776 | 0.997163 | 0.002837 | 00:00 | . 5 | 0.217856 | 0.025257 | 0.997163 | 0.002837 | 00:00 | . 6 | 0.169902 | 0.018588 | 0.997163 | 0.002837 | 00:00 | . 7 | 0.134621 | 0.014637 | 0.998582 | 0.001418 | 00:00 | . 8 | 0.107967 | 0.012024 | 0.998582 | 0.001418 | 00:00 | . 9 | 0.087427 | 0.010147 | 0.998582 | 0.001418 | 00:00 | . fastai&#51648;&#50896; &#47700;&#53944;&#47533;&#51060;&#50857;-- &#50732;&#48148;&#47480; &#49324;&#50857;(2) . - 가정 . X의 형태는 (n,채널,픽셀,픽셀)로 가정한다. | y의 형태는 (n,클래스의수)로 가정한다. 즉 y가 one_hot 인코딩된 형태로 가정한다. | y의 각 원소는 0 혹은 1이다. | loss function은 CrossEntropyLoss()를 쓴다고 가정한다. (따라서 네트워크의 최종레이어는 torch.nn.Linear(?,클래스의수) 꼴이 되어야 한다.) | . (1) dls 만들기 . y_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y))) yy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy))) # y_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32) # yy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32) . ds1 = torch.utils.data.TensorDataset(X,y_onehot) ds2 = torch.utils.data.TensorDataset(XX,yy_onehot) dl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) dl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) dls = DataLoaders(dl1,dl2) . (2) lrnr 생성 . net = torch.nn.Sequential( torch.nn.Conv2d(1,16,(5,5)), torch.nn.ReLU(), torch.nn.MaxPool2d((2,2)), torch.nn.Flatten(), torch.nn.Linear(2304,2), #torch.nn.Softmax() ) loss_fn = torch.nn.CrossEntropyLoss() lrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi]) . (3) 학습 . lrnr.fit(10) . epoch train_loss valid_loss accuracy_multi time . 0 | 1.073728 | 0.608127 | 0.463357 | 00:00 | . 1 | 0.683059 | 0.328684 | 0.982979 | 00:00 | . 2 | 0.508058 | 0.156030 | 0.990780 | 00:00 | . 3 | 0.378242 | 0.071232 | 0.995272 | 00:00 | . 4 | 0.284064 | 0.041114 | 0.995981 | 00:00 | . 5 | 0.217714 | 0.027780 | 0.996217 | 00:00 | . 6 | 0.170183 | 0.021023 | 0.995981 | 00:00 | . 7 | 0.135240 | 0.016962 | 0.996927 | 00:00 | . 8 | 0.108881 | 0.014204 | 0.997400 | 00:00 | . 9 | 0.088585 | 0.012181 | 0.997872 | 00:00 | .",
            "url": "https://boram-coco.github.io/coco1/2022/10/26/(8%EC%A3%BC%EC%B0%A8)-10%EC%9B%9426%EC%9D%BC(1).html",
            "relUrl": "/2022/10/26/(8%EC%A3%BC%EC%B0%A8)-10%EC%9B%9426%EC%9D%BC(1).html",
            "date": " • Oct 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(6주차) 10월12일 -- draft",
            "content": "&#44053;&#51032;&#50689;&#49345; . imports . import torch import torchvision from fastai.data.all import * import matplotlib.pyplot as plt . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;); . &#49884;&#48292;&#53076;&#51221;&#47532; . &#51648;&#45212;&#49884;&#44036; &#45436;&#47532;&#51204;&#44060; . - 아이디어: linear -&gt; relu -&gt; linear (-&gt; sigmoid) 조합으로 꺽은선으로 표현되는 underlying 을 표현할 수 있었다. . 아이디어의 실용성: 실제자료에서 꺽은선으로 표현되는 underlying은 몇개 없을 것 같음. 그건 맞는데 꺽이는 점을 많이 설정하면 얼추 비슷하게는 &quot;근사&quot; 시킬 수 있음. | 아이디어의 확장성: 이러한 논리전개는 X:(n,2)인 경우도 가능했음. (이 경우 꺽인선은 꺽인평면이 된다) | 아이디어에 해당하는 용어정리: 이 구조가 x-&gt;y 로 바로 가는 것이 아니라 x-&gt;(u1-&gt;v1)-&gt;(u2-&gt;v2)=y 의 구조인데 이러한 네트워크를 하나의 은닉층을 포함하는 네트워크라고 표현한다. (이 용어는 이따가..) | . &#49884;&#48292;&#53076;&#51221;&#47532; . universal approximation thm: (범용근사정리,보편근사정리,시벤코정리), 1989 . 하나의 은닉층을 가지는 &quot;linear -&gt; sigmoid -&gt; linear&quot; 꼴의 네트워크를 이용하여 세상에 존재하는 모든 (다차원) 연속함수를 원하는 정확도로 근사시킬 수 있다. (계수를 잘 추정한다면) . - 사실 엄청 이해안되는 정리임. 왜냐햐면, . 그렇게 잘 맞추면 1989년에 세상의 모든 문제를 다 풀어야 한거 아니야? | 요즘은 &quot;linear -&gt; sigmoid -&gt; linear&quot; 가 아니라 &quot;linear -&gt; relu -&gt; linear&quot; 조합으로 많이 쓰던데? | 요즘은 하나의 은닉층을 포함하는 네트워크는 잘 안쓰지 않나? 은닉층이 여러개일수록 좋다고 어디서 본 것 같은데? | . - 약간의 의구심이 있지만 아무튼 universal approximation thm에 따르면 우리는 아래와 같은 무기를 가진 꼴이 된다. . 우리의 무기: ${ bf X}: (n,p)$ 꼴의 입력에서 ${ bf y}:(n,1)$ 꼴의 출력으로 향하는 맵핑을 &quot;linear -&gt; relu -&gt; linear&quot;와 같은 네트워크를 이용해서 &quot;근사&quot;시킬 수 있다. | . &#49884;&#48292;&#53076;&#51221;&#47532; proof . &#44536;&#47548;&#51004;&#47196; &#48372;&#45716; &#51613;&#47749;&#44284;&#51221; . - 데이터 . x = torch.linspace(-10,10,200).reshape(-1,1) . - 아래와 같은 네트워크를 고려하자. . l1 = torch.nn.Linear(in_features=1,out_features=2) a1 = torch.nn.Sigmoid() l2 = torch.nn.Linear(in_features=2,out_features=1) . - 직관1: $l_1$,$l_2$의 가중치를 잘 결합하다보면 우연히 아래와 같이 만들 수 있다. . l1.weight.data = torch.tensor([[-5.00],[5.00]]) l1.bias.data = torch.tensor([+10.00,+10.00]) . l2.weight.data = torch.tensor([[1.00,1.00]]) l2.bias.data = torch.tensor([-1.00]) . fig,ax = plt.subplots(1,3,figsize=(9,3)) ax[0].plot(x,l1(x).data); ax[0].set_title(&#39;$l_1(x)$&#39;) ax[1].plot(x,a1(l1(x)).data); ax[1].set_title(&#39;$(a_1 circ l_1)(x)$&#39;) ax[2].plot(x,l2(a1(l1(x))).data,color=&#39;C2&#39;); ax[2].set_title(&#39;$(l_2 circ a_1 circ l_1)(x)$&#39;) #모자의 시프팅(왼족아래쪽이동), 모자높이 이동 (1-&gt;2...) . Text(0.5, 1.0, &#39;$(l_2 circ a_1 circ l_1)(x)$&#39;) . - 직관2: 아래들도 가능할듯? . l1.weight.data = torch.tensor([[-5.00],[5.00]]) l1.bias.data = torch.tensor([+0.00,+20.00]) l2.weight.data = torch.tensor([[1.00,1.00]]) l2.bias.data = torch.tensor([-1.00]) fig,ax = plt.subplots(1,3,figsize=(9,3)) ax[0].plot(x,l1(x).data,&#39;--&#39;,color=&#39;C0&#39;); ax[0].set_title(&#39;$l_1(x)$&#39;) ax[1].plot(x,a1(l1(x)).data,&#39;--&#39;,color=&#39;C0&#39;); ax[1].set_title(&#39;$(a_1 circ l_1)(x)$&#39;) ax[2].plot(x,l2(a1(l1(x))).data,&#39;--&#39;,color=&#39;C0&#39;); ax[2].set_title(&#39;$(l_2 circ a_1 circ l_1)(x)$&#39;); . l1.weight.data = torch.tensor([[-5.00],[5.00]]) l1.bias.data = torch.tensor([+20.00,+0.00]) l2.weight.data = torch.tensor([[2.50,2.50]]) l2.bias.data = torch.tensor([-2.50]) ax[0].plot(x,l1(x).data,&#39;--&#39;,color=&#39;C1&#39;); ax[0].set_title(&#39;$l_1(x)$&#39;) ax[1].plot(x,a1(l1(x)).data,&#39;--&#39;,color=&#39;C1&#39;); ax[1].set_title(&#39;$(a_1 circ l_1)(x)$&#39;) ax[2].plot(x,l2(a1(l1(x))).data,&#39;--&#39;,color=&#39;C1&#39;); ax[2].set_title(&#39;$(l_2 circ a_1 circ l_1)(x)$&#39;); fig . - 은닉층의노드수=4로 하고 적당한 가중치를 조정하면 $(l_2 circ a_1 circ l_1)(x)$의 결과로 주황색선 + 파란색선도 가능할 것 같다. $ to$ 실제로 가능함 . l1 = torch.nn.Linear(in_features=1,out_features=4) a1 = torch.nn.Sigmoid() l2 = torch.nn.Linear(in_features=4,out_features=1) . l1.weight.data = torch.tensor([[-5.00],[5.00],[-5.00],[5.00]]) l1.bias.data = torch.tensor([0.00, 20.00, 20.00, 0]) l2.weight.data = torch.tensor([[1.00, 1.00, 2.50, 2.50]]) l2.bias.data = torch.tensor([-1.0-2.5]) . plt.plot(l2(a1(l1(x))).data) . [&lt;matplotlib.lines.Line2D at 0x7f14f408ad90&gt;] . - 2개의 시그모이드를 우연히 잘 결합하면 아래와 같은 함수 $h$를 만들 수 있다. . h = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0 . plt.plot(x,h(x)) plt.title(&quot;$h(x)$&quot;) . Text(0.5, 1.0, &#39;$h(x)$&#39;) . - 위와 같은 함수 $h$를 활성화함수로 하고 $m$개의 노드를 가지는 은닉층을 생각해보자. 이러한 은닉층을 사용한다면 전체 네트워크를 아래와 같이 표현할 수 있다. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{h}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ . . 그리고 위의 네트워크와 동일한 효과를 주는 아래의 네트워크가 항상 존재함. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,2m)}{ boldsymbol u^{(1)}} overset{sig}{ to} underset{(n,2m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ . - $h(x)$를 활성화함수로 가지는 네트워크를 설계하여 보자. . class MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법 def __init__(self): super().__init__() def forward(self, input): #forward: x에서 y로가는거.. return h(input) # activation 의 출력 . a1=MyActivation() # a1 = torch.nn.Sigmoid(), a1 = torch.nn.ReLU() 대신에 a1 = MyActivation() . plt.plot(x,a1(x)) . [&lt;matplotlib.lines.Line2D at 0x7f62d8043b10&gt;] . 히든레이어가 1개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,1), MyActivation(), torch.nn.Linear(1,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . 히든레이어가 2개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,2), MyActivation(), torch.nn.Linear(2,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . 히든레이어가 3개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,3), MyActivation(), torch.nn.Linear(3,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . 히든레이어가 1024개의 노드를 가지는 경우 . torch.manual_seed(43052) fig, ax = plt.subplots(4,4,figsize=(12,12)) for i in range(4): for j in range(4): net = torch.nn.Sequential( torch.nn.Linear(1,1024), MyActivation(), torch.nn.Linear(1024,1) ) ax[i,j].plot(x,net(x).data,&#39;--&#39;) . &#49884;&#48292;&#53076;&#51221;&#47532; &#54876;&#50857; . - 아래와 같이 하나의 은닉층을 가지고 있더라도 많은 노드수만 보장되면 매우 충분한 표현력을 가짐 . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{h}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ . &#50696;&#51228;1 (sin, exp) . torch.manual_seed(43052) x = torch.linspace(-10,10,200).reshape(-1,1) underlying = torch.sin(2*x) + torch.sin(0.5*x) + torch.exp(-0.2*x) eps = torch.randn(200).reshape(-1,1)*0.1 #오차항 y = underlying + eps plt.plot(x,y,&#39;o&#39;,alpha=0.5) plt.plot(x,underlying,lw=3) . [&lt;matplotlib.lines.Line2D at 0x7f62d9f82550&gt;] . h = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0 class MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법 def __init__(self): super().__init__() def forward(self, input): return h(input) . net= torch.nn.Sequential( torch.nn.Linear(1,2048), MyActivation(), torch.nn.Linear(2048,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.2) plt.plot(x,underlying,lw=3) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f62d9bc3d90&gt;] . &#50696;&#51228;2 (&#49828;&#54169;&#45458;&#50500;&#46020; &#52712;&#50629;X) . df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv&#39;) df . x underlying y . 0 -1.000000 | 0.000045 | 0.0 | . 1 -0.998999 | 0.000046 | 0.0 | . 2 -0.997999 | 0.000047 | 0.0 | . 3 -0.996998 | 0.000047 | 0.0 | . 4 -0.995998 | 0.000048 | 0.0 | . ... ... | ... | ... | . 1995 0.995998 | 0.505002 | 0.0 | . 1996 0.996998 | 0.503752 | 0.0 | . 1997 0.997999 | 0.502501 | 0.0 | . 1998 0.998999 | 0.501251 | 1.0 | . 1999 1.000000 | 0.500000 | 1.0 | . 2000 rows × 3 columns . x = torch.tensor(df.x).reshape(-1,1).float() y = torch.tensor(df.y).reshape(-1,1).float() plt.plot(x,y,&#39;o&#39;,alpha=0.1) plt.plot(df.x,df.underlying,lw=3) . [&lt;matplotlib.lines.Line2D at 0x7f62d97965d0&gt;] . h = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0 class MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법 def __init__(self): super().__init__() def forward(self, input): return h(input) . torch.manual_seed(43052) net= torch.nn.Sequential( torch.nn.Linear(1,2048), MyActivation(), torch.nn.Linear(2048,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(100): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.2) plt.plot(df.x,df.underlying,lw=3) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f62d971fd90&gt;] . &#50696;&#51228;3 (MNIST data with DNN) . # &#50696;&#48708;&#54617;&#49845; . (예비학습1) Path . path = untar_data(URLs.MNIST) path . . 100.03% [15687680/15683414 00:00&lt;00:00] Path(&#39;/root/.fastai/data/mnist_png&#39;) . path 도 오브젝트임 | path 도 정보+기능이 있음 | . - path의 정보 . path._str # 숨겨놓았네? #path도 object 동작을 정의하는 기능이 있을거야.. #path 오브젝트에 저장된 정보(attribute, 기능은 method) . &#39;/root/.fastai/data/mnist_png&#39; . - 기능1 . path.ls() # path 오브젝트의 안에 있는 목록(폴더)를 보여줘! . (#2) [Path(&#39;/root/.fastai/data/mnist_png/training&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing&#39;)] . - 기능2 . path/&#39;training&#39; #경로를 결합 . Path(&#39;/root/.fastai/data/mnist_png/training&#39;) . path/&#39;testing&#39; . Path(&#39;/root/.fastai/data/mnist_png/testing&#39;) . - 기능1과 기능2의 결합 . (path/&#39;training/3&#39;).ls() . (#6131) [Path(&#39;/root/.fastai/data/mnist_png/training/3/53954.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/training/3/26033.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/training/3/50212.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/training/3/7192.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/training/3/15045.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/training/3/9268.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/training/3/46899.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/training/3/3609.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/training/3/12729.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/training/3/23700.png&#39;)...] . &#39;/home/cgb4/.fastai/data/mnist_png/training/3/37912.png&#39; 이 파일을 더블클릭하면 이미지가 보인단 말임 | . (예비학습2) plt.imshow . . imgtsr = torch.tensor([[0.1,0.2],[0.3,0.4]]) imgtsr . tensor([[0.1000, 0.2000], [0.3000, 0.4000]]) . plt.imshow(imgtsr,cmap=&#39;gray&#39;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7f14f2a39250&gt; . (예비학습3) torchvision . - &#39;/home/cgb4/.fastai/data/mnist_png/training/3/37912.png&#39;의 이미지파일을 torchvision.io.read_image 를 이용하여 텐서로 만듬 . #ls아닌가? 뭐지 . ls: cannot access &#39;/home/cgb4/.fastai/data/mnist_png/training/3&#39;: No such file or directory . imgtsr = torchvision.io.read_image(&#39;/home/cgb4/.fastai/data/mnist_png/training/3/37912.png&#39;) imgtsr . tensor([[[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 66, 138, 149, 180, 138, 138, 86, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 22, 162, 161, 228, 252, 252, 253, 252, 252, 252, 252, 74, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 116, 253, 252, 252, 252, 189, 184, 110, 119, 252, 252, 32, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 74, 161, 160, 77, 45, 4, 0, 0, 70, 252, 210, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 205, 252, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 162, 253, 245, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 219, 252, 139, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 222, 252, 202, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 43, 253, 252, 89, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 85, 240, 253, 157, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 160, 253, 231, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 142, 252, 252, 42, 30, 78, 161, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 184, 252, 252, 185, 228, 252, 252, 168, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 184, 252, 252, 253, 252, 252, 252, 116, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 101, 179, 252, 253, 252, 252, 210, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 255, 253, 215, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 34, 89, 244, 253, 223, 98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 116, 123, 142, 234, 252, 252, 184, 67, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 230, 253, 252, 252, 252, 168, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 126, 253, 252, 168, 43, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.uint8) . - 이 텐서는 (1,28,28)의 shape을 가짐 . imgtsr.shape . torch.Size([1, 28, 28]) . . - imgtsr를 plt.imshow 로 시각화 . plt.imshow(imgtsr.reshape(28,28),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7fceabd49a90&gt; . 진짜 숫자3이 있음 | . # &#45936;&#51060;&#53552;&#51221;&#47532; . - 데이터정리 . threes = (path/&#39;training/3&#39;).ls() #6131개, 1,28,28 sevens = (path/&#39;training/7&#39;).ls() #6265개, 1,28,28 len(threes),len(sevens) . (6131, 6265) . X3 = torch.stack([torchvision.io.read_image(str(threes[i])) for i in range(6131)]) #리스트 형태로 만들고.. X7 = torch.stack([torchvision.io.read_image(str(sevens[i])) for i in range(6265)]) . # X7 = torch.stack([torchvision.io.read_image(str(fn)])) for i in seven_fnames]) # 위와 같은 코드 . X3.shape,X7.shape . (torch.Size([6131, 1, 28, 28]), torch.Size([6265, 1, 28, 28])) . X=torch.concat([X3,X7]) X.shape . torch.Size([12396, 1, 28, 28]) . Xnp = X.reshape(-1,1*28*28).float() #n * p 의 shape #float로 바꿔줘야함 Xnp.shape . torch.Size([12396, 784]) . y = torch.tensor([0.0]*6131 + [1.0]*6265).reshape(-1,1) # 3을 0으로 7을 1로.. 나중에 sigmoin하기 편하게 하려고 6131대신에 len(X3)이렇게 써도 됨 y.shape . torch.Size([12396, 1]) . plt.plot(y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f14f25f4d50&gt;] . &quot;y=0&quot;은 숫자3을 의미, &quot;y=1&quot;은 숫자7을 의미 | 숫자3은 6131개, 숫자7은 6265개 있음 | . # &#54617;&#49845; . - 네트워크의 설계 . torch.manual_seed(43052) net = torch.nn.Sequential( #묶어주기.. torch.nn.Linear(in_features=1*28*28,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1), torch.nn.Sigmoid() ) . $ underset{(n,784)}{ bf X} overset{l_1}{ to} underset{(n,30)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,30)}{ boldsymbol v^{(1)}} overset{l_1}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ | . loss_fn = torch.nn.BCELoss() . optimizr = torch.optim.Adam(net.parameters()) . plt.plot(y,&#39;o&#39;) plt.plot(net(Xnp).data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f14f1fbce10&gt;] . for epoc in range(200): ## 1 yhat = net(Xnp) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(y,&#39;o&#39;) plt.plot(net(Xnp).data,&#39;.&#39;,alpha=0.2) . [&lt;matplotlib.lines.Line2D at 0x7f14f20b8a90&gt;] . 대부분 잘 적합되었음 | . &#49888;&#44221;&#47581;&#51032; &#54364;&#54788; (${ boldsymbol x} to hat{ boldsymbol y}$ &#47196; &#44032;&#45716; &#44284;&#51221;&#51012; &#44536;&#47548;&#51004;&#47196; &#54364;&#54788;) . &#50696;&#51228;1: $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,1)}{ boldsymbol u^{(1)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(1)}} = underset{(n,1)}{ hat{ boldsymbol y}}$ . - 모든 observation과 가중치를 명시한 버전 . (표현1) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;ŵ₀ + xₙ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;xₙ&quot; -&gt; &quot;ŵ₀ + xₙ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + xₙ*ŵ₁, bias=False&quot; -&gt; &quot;ŷₙ&quot;[label=&quot;sigmoid&quot;] &quot;.&quot; -&gt; &quot;....................................&quot;[label=&quot;* ŵ₀&quot;] &quot;..&quot; -&gt; &quot;....................................&quot;[label=&quot;* ŵ₁&quot;] &quot;....................................&quot; -&gt; &quot;...&quot;[label=&quot; &quot;] &quot;1 &quot; -&gt; &quot;ŵ₀ + x₂*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;x₂&quot; -&gt; &quot;ŵ₀ + x₂*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + x₂*ŵ₁, bias=False&quot; -&gt; &quot;ŷ₂&quot;[label=&quot;sigmoid&quot;] &quot;1 &quot; -&gt; &quot;ŵ₀ + x₁*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;x₁&quot; -&gt; &quot;ŵ₀ + x₁*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + x₁*ŵ₁, bias=False&quot; -&gt; &quot;ŷ₁&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . 단점: 똑같은 그림의 반복이 너무 많음 | . - observation 반복을 생략한 버전들 . (표현2) 모든 $i$에 대하여 아래의 그림을 반복한다고 하면 (표현1)과 같다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;ŵ₀ + xᵢ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;xᵢ&quot; -&gt; &quot;ŵ₀ + xᵢ*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + xᵢ*ŵ₁, bias=False&quot; -&gt; &quot;ŷᵢ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . (표현3) 그런데 (표현2)에서 아래와 같이 $x_i$, $y_i$ 대신에 간단히 $x$, $y$로 쓰는 경우도 많음 . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;ŵ₀ + x*ŵ₁, bias=False&quot;[label=&quot;* ŵ₀&quot;] &quot;x&quot; -&gt; &quot;ŵ₀ + x*ŵ₁, bias=False&quot;[label=&quot;* ŵ₁&quot;] &quot;ŵ₀ + x*ŵ₁, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . - 1을 생략한 버전들 . (표현4) bais=False 대신에 bias=True를 주면 1을 생략할 수 있음 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*ŵ₁, bias=True&quot;[label=&quot;*ŵ₁&quot;] ; &quot;x*ŵ₁, bias=True&quot; -&gt; &quot;ŷ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . (표현4의 수정) $ hat{w}_1$대신에 $ hat{w}$를 쓰는 것이 더 자연스러움 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*ŵ, bias=True&quot;[label=&quot;*ŵ&quot;] ; &quot;x*ŵ, bias=True&quot; -&gt; &quot;ŷ&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . (표현5) 선형변환의 결과는 아래와 같이 $u$로 표현하기도 한다. . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;u&quot;; &quot;u&quot; -&gt; &quot;y&quot;[label=&quot;sigmoid&quot;] &#39;&#39;&#39;) . . 다이어그램은 그리는 사람의 취향에 따라 그리는 방법이 조금씩 다릅니다. 즉 교재마다 달라요. . &#50696;&#51228;2: $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,2)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,2)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(2)}} = underset{(n,1)}{ hat{ boldsymbol y}}$ . 참고: 코드로 표현 . torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=2), torch.nn.ReLU(), torch.nn.Linear(in_features=2,out_features=1), torch.nn.Sigmoid() ) . - 이해를 위해서 10월4일 강의노트에서 다루었던 아래의 상황을 고려하자. . . (강의노트의 표현) . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot; -x&quot;[label=&quot;*(-1)&quot;]; &quot;x&quot; -&gt; &quot; x&quot;[label=&quot;*1&quot;] &quot; x&quot; -&gt; &quot;rlu(x)&quot;[label=&quot;relu&quot;] &quot; -x&quot; -&gt; &quot;rlu(-x)&quot;[label=&quot;relu&quot;] &quot;rlu(x)&quot; -&gt; &quot;u&quot;[label=&quot;*(-4.5)&quot;] &quot;rlu(-x)&quot; -&gt; &quot;u&quot;[label=&quot;*(-9.0)&quot;] &quot;u&quot; -&gt; &quot;sig(u)=yhat&quot;[label=&quot;sig&quot;] &#39;&#39;&#39; ) . . (좀 더 일반화된 표현) 10월4일 강의노트 상황을 일반화하면 아래와 같다. . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;u1[:,0]&quot;[label=&quot;*(-1)&quot;]; &quot;x&quot; -&gt; &quot;u1[:,1]&quot;[label=&quot;*1&quot;] &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;relu&quot;] &quot;v1[:,0]&quot; -&gt; &quot;u2&quot;[label=&quot;*(-9.0)&quot;] &quot;v1[:,1]&quot; -&gt; &quot;u2&quot;[label=&quot;*(-4.5)&quot;] &quot;u2&quot; -&gt; &quot;v2=yhat&quot;[label=&quot;sig&quot;] &#39;&#39;&#39; ) . . * Layer의 개념: ${ bf X}$에서 $ hat{ boldsymbol y}$로 가는 과정은 &quot;선형변환+비선형변환&quot;이 반복되는 구조이다. &quot;선형변환+비선형변환&quot;을 하나의 세트로 보면 아래와 같이 표현할 수 있다. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} left( underset{(n,2)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,2)}{ boldsymbol v^{(1)}} right) overset{l_2}{ to} left( underset{(n,1)}{ boldsymbol u^{(2)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(2)}} right), quad underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{net({ bf X})}= underset{(n,1)}{ hat{ boldsymbol y}}$ . 이것을 다이어그램으로 표현한다면 아래와 같다. . (선형+비선형을 하나의 Layer로 묶은 표현) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;u1[:,0]&quot; &quot;X&quot; -&gt; &quot;u1[:,1]&quot; &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;relu&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;v1[:,0]&quot; -&gt; &quot;u2&quot; &quot;v1[:,1]&quot; -&gt; &quot;u2&quot; &quot;u2&quot; -&gt; &quot;v2=yhat&quot;[label=&quot;sigmoid&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . Layer를 세는 방법 . 정석: 학습가능한 파라메터가 몇층으로 있는지... | 일부 교재 설명: 입력층은 계산하지 않음, activation layer는 계산하지 않음. | 위의 예제의 경우 number of layer = 2 이다. | . 사실 input layer, activation layer 등의 표현을 자주 사용해서 layer를 세는 방법이 처음에는 헷갈립니다.. . Hidden Layer의 수를 세는 방법 . Layer의 수 = Hidden Layer의 수 + 출력층의 수 = Hidden Layer의 수 + 1 | 위의 예제의 경우 number of hidden layer = 1 이다. | . * node의 개념: $u to v$로 가는 쌍을 간단히 노드라는 개념을 이용하여 나타낼 수 있음. . (노드의 개념이 포함된 그림) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; label = &quot;Layer 1:relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat &quot; &quot;node2&quot; -&gt; &quot;yhat &quot; label = &quot;Layer 2:sigmoid&quot; } &#39;&#39;&#39;) . . 여기에서 node의 숫자 = feature의 숫자와 같이 이해할 수 있다. 즉 아래와 같이 이해할 수 있다. . (&quot;number of nodes = number of features&quot;로 이해한 그림) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;feature1&quot; &quot;X&quot; -&gt; &quot;feature2&quot; label = &quot;Layer 1:relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;feature1&quot; -&gt; &quot;yhat &quot; &quot;feature2&quot; -&gt; &quot;yhat &quot; label = &quot;Layer 2:sigmoid&quot; } &#39;&#39;&#39;) . . 다이어그램의 표현방식은 교재마다 달라서 모든 예시를 달달 외울 필요는 없습니다. 다만 임의의 다이어그램을 보고 대응하는 네트워크를 pytorch로 구현하는 능력은 매우 중요합니다. . &#50696;&#51228;3: $ underset{(n,784)}{ bf X} overset{l_1}{ to} underset{(n,32)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,32)}{ boldsymbol v^{(1)}} overset{l_1}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{sig}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ . (다이어그램표현) . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Input Layer&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node32&quot; &quot;x2&quot; -&gt; &quot;node32&quot; &quot;..&quot; -&gt; &quot;node32&quot; &quot;x784&quot; -&gt; &quot;node32&quot; label = &quot;Hidden Layer: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; &quot;...&quot; -&gt; &quot;yhat&quot; &quot;node32&quot; -&gt; &quot;yhat&quot; label = &quot;Outplut Layer: sigmoid&quot; } &#39;&#39;&#39;) . . Layer0,1,2 대신에 Input Layer, Hidden Layer, Output Layer로 표현함 | . - 위의 다이어그램에 대응하는 코드 . net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28*1,out_features=32), torch.nn.ReLU(), torch.nn.Linear(in_features=32,out_features=1), torch.nn.Sigmoid() ) . CPU vs GPU . - 파이토치에서 GPU를 쓰는 방법을 알아보자. (사실 지금까지 우리는 CPU만 쓰고 있었음) . GPU &#49324;&#50857;&#48169;&#48277; . - cpu 연산이 가능한 메모리에 데이터 저장 . torch.manual_seed(43052) x_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1) #net에 넣어야니까 shape을 바꿔주기 y_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1) net_cpu = torch.nn.Linear(1,1) . - gpu 연산이 가능한 메모리에 데이터 저장 . torch.manual_seed(43052) x_gpu = x_cpu.to(&quot;cuda:0&quot;) y_gpu = y_cpu.to(&quot;cuda:0&quot;) net_gpu = torch.nn.Linear(1,1).to(&quot;cuda:0&quot;) #net_cpu.to(&quot;cuda:0&quot;) 하게 되면 net_cpu도 gpu로 넘어가게 되므로 그대로 써주면 안뎀 . - cpu 혹은 gpu 연산이 가능한 메모리에 저장된 값들을 확인 . x_cpu, y_cpu, net_cpu.weight, net_cpu.bias . (tensor([[0.0000], [0.1000], [0.2000]]), tensor([[0.0000], [0.2000], [0.4000]]), Parameter containing: tensor([[-0.3467]], requires_grad=True), Parameter containing: tensor([-0.8470], requires_grad=True)) . x_gpu, y_gpu, net_gpu.weight, net_gpu.bias . (tensor([[0.0000], [0.1000], [0.2000]], device=&#39;cuda:0&#39;), tensor([[0.0000], [0.2000], [0.4000]], device=&#39;cuda:0&#39;), Parameter containing: tensor([[-0.3467]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.8470], device=&#39;cuda:0&#39;, requires_grad=True)) . - gpu는 gpu끼리 연산가능하고 cpu는 cpu끼리 연산가능함 . (예시1) . net_cpu(x_cpu) . tensor([[-0.8470], [-0.8817], [-0.9164]], grad_fn=&lt;AddmmBackward0&gt;) . (예시2) . net_gpu(x_gpu) . tensor([[-0.8470], [-0.8817], [-0.9164]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward0&gt;) . (예시3) . net_cpu(x_gpu) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/955438640.py in &lt;module&gt; -&gt; 1 net_cpu(x_gpu) ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1129 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1130 return forward_call(*input, **kwargs) 1131 # Do not call functions when jit is used 1132 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input) 112 113 def forward(self, input: Tensor) -&gt; Tensor: --&gt; 114 return F.linear(input, self.weight, self.bias) 115 116 def extra_repr(self) -&gt; str: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm) . . (예시4) . net_gpu(x_cpu) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/413131407.py in &lt;module&gt; -&gt; 1 net_gpu(x_cpu) ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1129 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1130 return forward_call(*input, **kwargs) 1131 # Do not call functions when jit is used 1132 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input) 112 113 def forward(self, input: Tensor) -&gt; Tensor: --&gt; 114 return F.linear(input, self.weight, self.bias) 115 116 def extra_repr(self) -&gt; str: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm) . (예시5) . torch.mean((y_cpu-net_cpu(x_cpu))**2) . tensor(1.2068, grad_fn=&lt;MeanBackward0&gt;) . (예시6) . torch.mean((y_gpu-net_gpu(x_gpu))**2) . tensor(1.2068, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;) . (예시7) . torch.mean((y_gpu-net_cpu(x_cpu))**2) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/2449708258.py in &lt;module&gt; -&gt; 1 torch.mean((y_gpu-net_cpu(x_cpu))**2) RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! . (예시8) . torch.mean((y_cpu-net_gpu(x_gpu))**2) . RuntimeError Traceback (most recent call last) /tmp/ipykernel_1392421/2827309378.py in &lt;module&gt; -&gt; 1 torch.mean((y_cpu-net_gpu(x_gpu))**2) RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! . &#49884;&#44036;&#52769;&#51221; (&#50696;&#48708;&#54617;&#49845;) . import time . t1 = time.time() #현재시각 . t2 = time.time() . t2-t1 # 현재시간 - 현재시간 : 위아래 실행하는 만큼의 초 나옴 . 9.33418893814087 . CPU (512) . - 데이터준비 . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1) y=torch.randn(100).reshape(-1,1)*0.01 . - for문 준비 . net = torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Linear(512,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . - for문 + 학습시간측정 . t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 0.7906162738800049 . GPU (512) . - 데이터준비 . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1).to(&quot;cuda:0&quot;) y=(torch.randn(100).reshape(-1,1)*0.01).to(&quot;cuda:0&quot;) . RuntimeError Traceback (most recent call last) &lt;ipython-input-53-1c1e8228e0b9&gt; in &lt;module&gt; 1 torch.manual_seed(5) -&gt; 2 x=torch.linspace(0,1,100).reshape(-1,1).to(&#34;cuda:0&#34;) 3 y=(torch.randn(100).reshape(-1,1)*0.01).to(&#34;cuda:0&#34;) /usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py in _lazy_init() 215 # This function throws if there&#39;s a driver initialization error, no GPUs 216 # are found or any other error occurs --&gt; 217 torch._C._cuda_init() 218 # Some of the queued calls may reentrantly call _lazy_init(); 219 # we need to just return without initializing in that case. RuntimeError: No CUDA GPUs are available . - for문돌릴준비 . net = torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Linear(512,1) ).to(&quot;cuda:0&quot;) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) . - for문 + 학습시간측정 . t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 0.5355696678161621 . !! CPU가 더 빠르다? | . CPU vs GPU (20480) . - CPU (20480) . . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1) y=torch.randn(100).reshape(-1,1)*0.01 net = torch.nn.Sequential( torch.nn.Linear(1,20480), torch.nn.ReLU(), torch.nn.Linear(20480,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 2.380666494369507 . - GPU (20480) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1).to(&quot;cuda:0&quot;) y=(torch.randn(100).reshape(-1,1)*0.01).to(&quot;cuda:0&quot;) net = torch.nn.Sequential( torch.nn.Linear(1,20480), torch.nn.ReLU(), torch.nn.Linear(20480,1) ).to(&quot;cuda:0&quot;) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 0.5442469120025635 . - 왜 이런 차이가 나는가? 연산을 하는 주체는 코어인데 CPU는 수는 적지만 일을 잘하는 코어들을 가지고 있고 GPU는 일은 못하지만 다수의 코어를 가지고 있기 때문 . CPU vs GPU (204800) . - CPU (204800) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1) y=torch.randn(100).reshape(-1,1)*0.01 net = torch.nn.Sequential( torch.nn.Linear(1,204800), torch.nn.ReLU(), torch.nn.Linear(204800,1) ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 51.95550894737244 . - GPU (204800) . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(-1,1).to(&quot;cuda:0&quot;) y=(torch.randn(100).reshape(-1,1)*0.01).to(&quot;cuda:0&quot;) net = torch.nn.Sequential( torch.nn.Linear(1,204800), torch.nn.ReLU(), torch.nn.Linear(204800,1) ).to(&quot;cuda:0&quot;) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters()) t1= time.time() for epoc in range(1000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() t2 = time.time() t2-t1 . 1.3824031352996826 . &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277;, &#48176;&#52824;, &#50640;&#54253; . &#51328; &#51060;&#49345;&#54616;&#51648; &#50506;&#50500;&#50836;? . - 우리가 쓰는 GPU: 다나와 PC견적 . GPU 메모리 끽해봐야 24GB | . - 우리가 분석하는 데이터: 빅데이터..? . - 데이터의 크기가 커지는순간 X.to(&quot;cuda:0&quot;), y.to(&quot;cuda:0&quot;) 쓰면 난리나겠는걸? . x = torch.linspace(-10,10,100000).reshape(-1,1) eps = torch.randn(100000).reshape(-1,1) y = x*2 + eps . plt.plot(x,y,&#39;o&#39;,alpha=0.05) plt.plot(x,2*x) . [&lt;matplotlib.lines.Line2D at 0x7f62d96f2c10&gt;] . - 데이터를 100개중에 1개만 꼴로만 쓰면 어떨까? . plt.plot(x[::100],y[::100],&#39;o&#39;,alpha=0.05) plt.plot(x,2*x) . [&lt;matplotlib.lines.Line2D at 0x7f62db305310&gt;] . 대충 이거만 가지고 적합해도 충분히 정확할것 같은데 | . X,y &#45936;&#51060;&#53552;&#47484; &#44403;&#51060; &#47784;&#46160; GPU&#50640; &#45336;&#44200;&#50556; &#54616;&#45716;&#44032;? . - 데이터셋을 짝홀로 나누어서 번갈아가면서 GPU에 올렸다 내렸다하면 안되나? . - 아래의 알고리즘을 생각해보자. . 데이터를 반으로 나눈다. | 짝수obs의 x,y 그리고 net의 모든 파라메터를 GPU에 올린다. | yhat, loss, grad, update 수행 | 짝수obs의 x,y를 GPU메모리에서 내린다. 그리고 홀수obs의 x,y를 GPU메모리에 올린다. | yhat, loss, grad, update 수행 | 홀수obs의 x,y를 GPU메모리에서 내린다. 그리고 짝수obs의 x,y를 GPU메모리에 올린다. | 반복 | &#44221;&#49324;&#54616;&#44053;&#48277;, &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277;, &#48120;&#45768;&#48176;&#52824; &#44221;&#49324;&#54616;&#44053;&#48277; . 10개의 샘플이 있다고 가정. $ {(x_i,y_i) }_{i=1}^{10}$ . - ver1: 모든 샘플을 이용하여 slope 계산 . (epoch1) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 to slope to update$ . (epoch2) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 to slope to update$ . ... . - ver2: 하나의 샘플만을 이용하여 slope 계산 . (epoch1) . $loss=(y_1- beta_0- beta_1x_1)^2 to slope to update$ | $loss=(y_2- beta_0- beta_1x_2)^2 to slope to update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . (epoch2) . $loss=(y_1- beta_0- beta_1x_1)^2 to slope to update$ | $loss=(y_2- beta_0- beta_1x_2)^2 to slope to update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . ... . - ver3: $m ( leq n)$ 개의 샘플을 이용하여 slope 계산 . $m=3$이라고 하자. . (epoch1) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . (epoch2) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 to slope to update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 to slope to update$ | . ... . &#50857;&#50612;&#51032; &#51221;&#47532; . 옛날 . - ver1: gradient descent, batch gradient descent . - ver2: stochastic gradient descent . - ver3: mini-batch gradient descent, mini-batch stochastic gradient descent . . 요즘 . - ver1: gradient descent . - ver2: stochastic gradient descent with batch size = 1 . - ver3: stochastic gradient descent . https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고. | . ds, dl . - ds . . x=torch.tensor(range(10)).float()#.reshape(-1,1) reshape원래는 해야는데 보여주기 위해서 생략쓰,, y=torch.tensor([1.0]*5+[0.0]*5)#.reshape(-1,1) . ds=torch.utils.data.TensorDataset(x,y) ds . &lt;torch.utils.data.dataset.TensorDataset at 0x7f62db294710&gt; . ds.tensors # 그냥 (x,y)의 튜플 . (tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]), tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])) . - dl . . dl=torch.utils.data.DataLoader(ds,batch_size=3) #batch_size: 3개씩 묶어서 배치를 해줌 #set(dir(dl)) &amp; {&#39;__iter__&#39;} #dir(dl):숨겨진 습성 #dl.__ : 숨겨진 습성 # iter 오브젝트: for문에 돌릴수 있다는 특성..! . for xx,yy in dl: #in 뒤에 iter오브젝트는 다 쓸수 있음 print(xx,yy) . tensor([0., 1., 2.]) tensor([1., 1., 1.]) tensor([3., 4., 5.]) tensor([1., 1., 0.]) tensor([6., 7., 8.]) tensor([0., 0., 0.]) tensor([9.]) tensor([0.]) . ds, dl&#51012; &#51060;&#50857;&#54620; MNIST &#44396;&#54788; . - 데이터정리 . path = untar_data(URLs.MNIST) . zero_fnames = (path/&#39;training/0&#39;).ls() one_fnames = (path/&#39;training/1&#39;).ls() . X0 = torch.stack([torchvision.io.read_image(str(zf)) for zf in zero_fnames]) X1 = torch.stack([torchvision.io.read_image(str(of)) for of in one_fnames]) X = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255 #255로나누는 이유 숙제 y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1) . X.shape,y.shape . (torch.Size([12665, 784]), torch.Size([12665, 1])) . - ds $ to$ dl . ds = torch.utils.data.TensorDataset(X,y) dl = torch.utils.data.DataLoader(ds,batch_size=2048) . 12665/2048 . 6.18408203125 . i = 0 for xx,yy in dl: # 총 7번 돌아가는 for문 print(i) i=i+1 . 0 1 2 3 4 5 6 . - 미니배치 안쓰는 학습 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(784,32), torch.nn.ReLU(), torch.nn.Linear(32,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(70): ## 1 yhat = net(X) ## 2 loss= loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . torch.sum((yhat&gt;0.5) == y) / len(y) # 분자: 전체 데이터 중 잘 맞춘게 몇개인지. # 분모: y의 갯수??? # torch.mean((yhat&gt;0.5) == y)*1.0) 계산하면 위와 같음 . tensor(0.9981) . # 1~2048, 2049~ 하면 6번 조금넘게나옴 (7번) . - 미니배치 쓰는 학습 (GPU 올리고 내리는 과정은 생략) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(784,32), torch.nn.ReLU(), torch.nn.Linear(32,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(10): for xx,yy in dl: ## 7번 ## 1 #yhat = net(xx) ## 2 loss = loss_fn(net(xx),yy) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . torch.mean(((net(X)&gt;0.5) == y)*1.0) . tensor(0.9950) . &#50724;&#48260;&#54588;&#54021; . - 오버피팅이란? . 위키: In mathematical modeling, overfitting is &quot;the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably&quot;. | 제 개념: 데이터를 &quot;데이터 = 언더라잉 + 오차&quot;라고 생각할때 우리가 데이터로부터 적합할 것은 언더라잉인데 오차항을 적합하고 있는 현상. | . &#50724;&#48260;&#54588;&#54021; &#50696;&#49884; . - $m$이 매우 클때 아래의 네트워크 거의 무엇이든 맞출수 있다고 보면 된다. . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{h}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ | $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{sig}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ | $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,m)}{ boldsymbol u^{(1)}} overset{relu}{ to} underset{(n,m)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ hat{ boldsymbol y}}$ | . - 그런데 종종 맞추지 말아야 할 것들도 맞춘다. . model: $y_i = (0 times x_i) + epsilon_i$, where $ epsilon_i sim N(0,0.01^2)$ . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7f62e48fc190&gt;] . y는 그냥 정규분포에서 생성한 오차이므로 $X to y$ 로 항햐는 규칙따위는 없음 | . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() for epoc in range(1000): ## 1 yhat=net(x) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(x,y) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f62e4933310&gt;] . 우리는 데이터를 랜덤에서 뽑았는데, 데이터의 추세를 따라간다 $ to$ 오버피팅 (underlying이 아니라 오차항을 따라가고 있음) | . &#50724;&#48260;&#54588;&#54021;&#51060;&#46972;&#45716; &#46748;&#47159;&#54620; &#51613;&#44144;! (train / test) . - 데이터의 분리하여 보자. . torch.manual_seed(5) x=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 xtr = x[:80] ytr = y[:80] xtest = x[80:] ytest = y[80:] plt.plot(xtr,ytr) plt.plot(xtest,ytest) plt.title(&#39;train: blue / test: orange&#39;); . - train만 학습 . torch.manual_seed(1) net1=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizr1= torch.optim.Adam(net1.parameters()) loss_fn= torch.nn.MSELoss() for epoc in range(1000): ## 1 # net(xtr) ## 2 loss=loss_fn(net1(xtr),ytr) ## 3 loss.backward() ## 4 optimizr1.step() optimizr1.zero_grad() . - training data로 학습한 net를 training data 에 적용 . plt.plot(x,y,alpha=0.5) plt.plot(xtr,net1(xtr).data,&#39;--&#39;) # prediction (train) . [&lt;matplotlib.lines.Line2D at 0x7f62de905190&gt;] . train에서는 잘 맞추는듯이 보인다. | . - training data로 학습한 net를 test data 에 적용 . plt.plot(x,y,alpha=0.5) plt.plot(xtr,net1(xtr).data,&#39;--&#39;) # prediction (train) plt.plot(xtest,net1(xtest).data,&#39;--&#39;) # prediction with unseen data (test) . [&lt;matplotlib.lines.Line2D at 0x7f62de084550&gt;] . train은 그럭저럭 따라가지만 test에서는 엉망이다. $ to$ overfit | . &#49689;&#51228; . 해설 및 풀이는 여기-10%EC%9B%946%EC%9D%BC.html#%EC%88%99%EC%A0%9C)참고 . (1) 숫자0과 숫자1을 구분하는 네트워크를 아래와 같은 구조로 설계하라 . $$ underset{(n,784)}{ bf X} overset{l_1}{ to} underset{(n,64)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,64)}{ boldsymbol v^{(1)}} overset{l_1}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$$ . 위에서 $a_1$은 relu를, $a_2$는 sigmoid를 의미한다. . &quot;y=0&quot;은 숫자0을 의미하도록 하고 &quot;y=1&quot;은 숫자1을 의미하도록 설정하라. | . (2) 아래의 지침에 따라 200 epoch 학습을 진행하라. . 손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss() 를 이용할 것. | 옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것. | . (3) 아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가? . 손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것. | 옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것. | . (4) 아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가? . 이미지의 값을 0과 1사이로 규격화 하라. (Xnp = Xnp/255 를 이용하세요!) | 손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것. | 옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것. | . (5) 아래와 같은 수식을 이용하여 accuracy를 계산하라. . $ text{accuracy}= frac{1}{n} sum_{i=1}^n I( tilde{y}_i=y_i)$ . $ tilde{y}_i = begin{cases} 1 &amp; hat{y}_i &gt; 0.5 0 &amp; hat{y}_i leq 0.5 end{cases}$ | $I( tilde{y}_i=y_i) = begin{cases} 1 &amp; tilde{y}_i=y_i 0 &amp; tilde{y}_i neq y_i end{cases}$ | . 단, $n$은 0과 1을 의미하는 이미지의 수 .",
            "url": "https://boram-coco.github.io/coco1/2022/10/12/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "relUrl": "/2022/10/12/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "date": " • Oct 12, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "(5주차) 10월5일 -- draft",
            "content": "imports . import torch import pandas as pd import numpy as np import matplotlib.pyplot as plt . &#49884;&#44033;&#54868;&#47484; &#50948;&#54620; &#51456;&#48708;&#54632;&#49688;&#46308; . 준비1 loss_fn을 plot하는 함수 . def plot_loss(loss_fn,ax=None): if ax==None: fig = plt.figure() ax=fig.add_subplot(1,1,1,projection=&#39;3d&#39;) ax.elev=15;ax.azim=75 w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.15),torch.arange(-1,10,0.15),indexing=&#39;ij&#39;) w0hat = w0hat.reshape(-1) w1hat = w1hat.reshape(-1) def l(w0hat,w1hat): yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x)) return loss_fn(yhat,y) loss = list(map(l,w0hat,w1hat)) ax.scatter(w0hat,w1hat,loss,s=0.1,alpha=0.2) ax.scatter(-1,5,l(-1,5),s=200,marker=&#39;*&#39;) # 실제로 -1,5에서 최소값을 가지는건 아님.. . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(-1+5x_i)}{1+ exp(-1+5x_i)}$ 에서 생성된 데이터 한정하여 손실함수가 그려지게 되어있음. | . 준비2: for문 대신 돌려주고 epoch마다 필요한 정보를 기록하는 함수 . def learn_and_record(net, loss_fn, optimizr): yhat_history = [] loss_history = [] what_history = [] for epoc in range(1000): ## step1 yhat = net(x) ## step2 loss = loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() ## record if epoc % 20 ==0: yhat_history.append(yhat.reshape(-1).data.tolist()) loss_history.append(loss.item()) what_history.append([net[0].bias.data.item(), net[0].weight.data.item()]) return yhat_history, loss_history, what_history . 20에폭마다 yhat, loss, what을 기록 | . 준비3: 애니메이션을 만들어주는 함수 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . def show_lrpr2(net,loss_fn,optimizr,suptitle=&#39;&#39;): yhat_history,loss_history,what_history = learn_and_record(net,loss_fn,optimizr) fig = plt.figure(figsize=(7,2.5)) ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ax1.set_xticks([]);ax1.set_yticks([]) ax2.set_xticks([]);ax2.set_yticks([]);ax2.set_zticks([]) ax2.elev = 15; ax2.azim = 75 ## ax1: 왼쪽그림 ax1.plot(x,v,&#39;--&#39;) ax1.scatter(x,y,alpha=0.05) line, = ax1.plot(x,yhat_history[0],&#39;--&#39;) plot_loss(loss_fn,ax2) fig.suptitle(suptitle) fig.tight_layout() def animate(epoc): line.set_ydata(yhat_history[epoc]) ax2.scatter(np.array(what_history)[epoc,0],np.array(what_history)[epoc,1],loss_history[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() return ani . 준비1에서 그려진 loss 함수위에, 준비2의 정보를 조합하여 애니메이션을 만들어주는 함수 | . Logistic intro (review + $ alpha$) . - 모델: $x$가 커질수록 $y=1$이 잘나오는 모형은 아래와 같이 설계할 수 있음 &lt; 외우세요!!! . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+ exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss= - sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ &lt; 외우세요!! . | . - toy example . x=torch.linspace(-1,1,2000).reshape(2000,1) w0= -1 w1= 5 u = w0+x*w1 v = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함 y = torch.bernoulli(v) . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4feed234d0&gt;] . note: $(w_0,w_1)$의 true는 $(-1,5)$이다. -&gt; $( hat{w}_0, hat{w}_1)$을 적당히 $(-1,5)$근처로 추정하면 된다는 의미 | . plt.scatter(x,y,alpha=0.05) plt.plot(x,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fee40ecd0&gt;] . - step1: yhat을 만들기 . (방법1) . torch.manual_seed(43052) l1 = torch.nn.Linear(1,1) #x의 shape보면(2000,1)인데 뒤에 1이 중요.. a1 = torch.nn.Sigmoid() yhat = a1(l1(x)) yhat . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . (방법2) . torch.manual_seed(43052) l1 = torch.nn.Linear(1,1) a1 = torch.nn.Sigmoid() net = torch.nn.Sequential(l1,a1) #net는 l1과 a1의 합성함수 yhat = net(x) yhat . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . (방법3) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) yhat = net(x) yhat # 단점: a1과 l1 각 통과하는게 궁금한데 이건 중간과정 보기가힘들다. # len(net) = 2 : 2가 나오네.. 우너소에 접근을 해보자 #net[0], net[1] . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . net[0] . Linear(in_features=1, out_features=1, bias=True) . net[0](x) . tensor([[-0.5003], [-0.5007], [-0.5010], ..., [-1.1930], [-1.1934], [-1.1937]], grad_fn=&lt;AddmmBackward0&gt;) . net[1] #a1의 기능 . Sigmoid() . net[1](net[0](x)) . tensor([[0.3775], [0.3774], [0.3773], ..., [0.2327], [0.2327], [0.2326]], grad_fn=&lt;SigmoidBackward0&gt;) . - step2: loss (일단 MSE로..) . (방법1) . torch.mean((y-yhat)**2) #mse는 교수님들이 싫어한데.. 왜? 몰라 일단 걍 해보쟈 . tensor(0.2846, grad_fn=&lt;MeanBackward0&gt;) . loss=torch.mean((y-yhat)**2) loss . tensor(0.2846, grad_fn=&lt;MeanBackward0&gt;) . (방법2) . loss_fn = torch.nn.MSELoss() loss_fn(yhat,y) # yhat을 먼저쓰자! . tensor(0.2846, grad_fn=&lt;MseLossBackward0&gt;) . - step3~4는 동일 . - 반복 (준비+for문) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=1,bias=True), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() #MSELoss로 하면.. .. 별로? BCE이거로바꾸기 optimizr = torch.optim.SGD(net.parameters(),lr=0.01) . plt.plot(x,y,&#39;o&#39;,alpha=0.01) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fec4d8090&gt;] . for epoc in range(3000): ## step1 yhat = net(x) ## step2 loss = loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() #청소 . plt.plot(x,y,&#39;o&#39;,alpha=0.05) plt.plot(x,v,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fec459e10&gt;] . &#47196;&#51648;&#49828;&#54001;--BCEloss . - BCEloss로 바꾸어서 적합하여 보자. . net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=1,bias=True), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.01) . for epoc in range(3000): ## step1 yhat = net(x) ## step2 loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) # loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.05) plt.plot(x,v,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fec2cb7d0&gt;] . - 왜 잘맞지? -&gt; &quot;linear -&gt; sigmoid&quot; 와 같은 net에 BCEloss를 이용하면 손실함수의 모양이 convex 하기 때문에 . . plot_loss 함수소개 = 이 예제에 한정하여 $ hat{w}_0, hat{w}_1,loss( hat{w}_0, hat{w}_1)$를 각각 $x,y,z$ 축에 그려줍니다. | . plot_loss(torch.nn.MSELoss()) . plot_loss(torch.nn.BCELoss()) . &#49884;&#44033;&#54868;1: MSE, &#51339;&#51008;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) #학습률 . l1,a1 = net #초기값 세팅 l1.bias.data = torch.tensor([-3.0]) #세팅값 l1.weight.data = torch.tensor([[-1.0]]) #세팅값 . show_lrpr2(net,loss_fn,optimizr,&#39;MSEloss, good_init // SGD&#39;) . &lt;/input&gt; Once Loop Reflect &#49884;&#44033;&#54868;2: MSE, &#45208;&#49244;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) . l1,a1 = net l1.bias.data = torch.tensor([-10.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;MSEloss, bad_init // SGD&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;3: BCE, &#51339;&#51008;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) . l1,a1 = net l1.bias.data = torch.tensor([-3.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;BCEloss, good_init // SGD&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;4: BCE, &#45208;&#49244;&#52488;&#44592;&#44050; . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.SGD(net.parameters(),lr=0.05) . l1,a1 = net l1.bias.data = torch.tensor([-3.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;BCEloss, good_init // SGD&#39;) . Once Loop Reflect &#47196;&#51648;&#49828;&#54001;--Adam (&#44397;&#48124;&#50741;&#54000;&#47560;&#51060;&#51200;) . # 1. 어려워서 몰라도 뎀 # 2. 가속도의 개념 . &#49884;&#44033;&#54868;1: MSE, &#51339;&#51008;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters(),lr=0.05) ## &lt;-- 여기를 수정! . l1,a1 = net l1.bias.data = torch.tensor([-3.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;MSEloss, good_init // Adam&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;2: MSE, &#45208;&#49244;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.MSELoss() optimizr = torch.optim.Adam(net.parameters(),lr=0.05) . l1,a1 = net l1.bias.data = torch.tensor([-10.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;MSEloss, bad_init // Adam&#39;) . Once Loop Reflect &#49884;&#44033;&#54868;3: BCE, &#51339;&#51008;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! (&#54844;&#51088;&#54644;&#48400;&#50836;..) . net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) loss_fn = torch.nn.BCELoss() optimizr = torch.optim.Adam(net.parameters(),lr=0.05) . l1,a1 = net l1.bias.data = torch.tensor([-3.0]) l1.weight.data = torch.tensor([[-1.0]]) . show_lrpr2(net,loss_fn,optimizr,&#39;BCEloss, good_init // Adam&#39;) . &lt;/input&gt; Once Loop Reflect &#49884;&#44033;&#54868;4: BCE, &#45208;&#49244;&#52488;&#44592;&#44050; --&gt; &#51060;&#44152; &#50500;&#45812;&#51004;&#47196;! (&#54844;&#51088;&#54644;&#48400;&#50836;..) . (참고) Adam이 우수한 이유? SGD보다 두 가지 측면에서 개선이 있었음. . 그런게 있음.. | 가속도의 개념을 적용!! | &#44618;&#51008;&#49888;&#44221;&#47581;--&#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#51032; &#54620;&#44228; . &#49888;&#47928;&#44592;&#49324; (&#45936;&#51060;&#53552;&#51032; &#47784;&#54000;&#48652;) . - 스펙이 높아도 취업이 안된다고 합니다.. . 중소·지방 기업 &quot;뽑아봤자 그만두니까&quot; . 중소기업 관계자들은 고스펙 지원자를 꺼리는 이유로 높은 퇴직률을 꼽는다. 여건이 좋은 대기업으로 이직하거나 회사를 관두는 경우가 많다는 하소연이다. 고용정보원이 지난 3일 공개한 자료에 따르면 중소기업 청년취업자 가운데 49.5%가 2년 내에 회사를 그만두는 것으로 나타났다. . 중소 IT업체 관계자는 &quot;기업 입장에서 가장 뼈아픈 게 신입사원이 그만둬서 새로 뽑는 일&quot;이라며 &quot;명문대 나온 스펙 좋은 지원자를 뽑아놔도 1년을 채우지 않고 그만두는 사원이 대부분이라 우리도 눈을 낮춰 사람을 뽑는다&quot;고 말했다. . &#44032;&#51676;&#45936;&#51060;&#53552; . - 위의 기사를 모티브로 한 데이터 . df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv&#39;) df . x underlying y . 0 -1.000000 | 0.000045 | 0.0 | . 1 -0.998999 | 0.000046 | 0.0 | . 2 -0.997999 | 0.000047 | 0.0 | . 3 -0.996998 | 0.000047 | 0.0 | . 4 -0.995998 | 0.000048 | 0.0 | . ... ... | ... | ... | . 1995 0.995998 | 0.505002 | 0.0 | . 1996 0.996998 | 0.503752 | 0.0 | . 1997 0.997999 | 0.502501 | 0.0 | . 1998 0.998999 | 0.501251 | 1.0 | . 1999 1.000000 | 0.500000 | 1.0 | . 2000 rows × 3 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; plt.plot(df.x,df.y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe719fa10&gt;] . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47196; &#51201;&#54633; . . x= torch.tensor(df.x).float().reshape(-1,1) #float(): 뒤에 거슬리는거 빼주기 y= torch.tensor(df.y).float().reshape(-1,1) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.Sigmoid() ) yhat=net(x) . loss_fn = torch.nn.BCELoss() loss = loss_fn(yhat,y) # loss = -torch.mean((y)*torch.log(yhat)+(1-y)*torch.log(1-yhat)) loss . tensor(0.9367, grad_fn=&lt;BinaryCrossEntropyBackward0&gt;) . optimizr = torch.optim.Adam(net.parameters()) . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) # 학습전 . [&lt;matplotlib.lines.Line2D at 0x7f4fe8ceab10&gt;] . for epoc in range(6000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;--b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe8d7bc50&gt;] . - 이건 epoc=6억번으로 설정해도 못 맞출 것 같다 (증가하다가 감소하는 underlying을 설계하는 것이 불가능) $ to$ 모형의 표현력이 너무 낮다. . &#54644;&#44208;&#52293; . - sigmoid 넣기 전의 상태가 꺽인 그래프 이어야 한다. . sig = torch.nn.Sigmoid() . fig,ax = plt.subplots(4,2,figsize=(8,8)) u1 = torch.tensor([-6,-4,-2,0,2,4,6]) u2 = torch.tensor([6,4,2,0,-2,-4,-6]) u3 = torch.tensor([-6,-2,2,6,2,-2,-6]) u4 = torch.tensor([-6,-2,2,6,4,2,0]) ax[0,0].plot(u1,&#39;--o&#39;,color=&#39;C0&#39;);ax[0,1].plot(sig(u1),&#39;--o&#39;,color=&#39;C0&#39;) ax[1,0].plot(u2,&#39;--o&#39;,color=&#39;C1&#39;);ax[1,1].plot(sig(u2),&#39;--o&#39;,color=&#39;C1&#39;) ax[2,0].plot(u3,&#39;--o&#39;,color=&#39;C2&#39;);ax[2,1].plot(sig(u3),&#39;--o&#39;,color=&#39;C2&#39;) ax[3,0].plot(u4,&#39;--o&#39;,color=&#39;C3&#39;);ax[3,1].plot(sig(u4),&#39;--o&#39;,color=&#39;C3&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe976bfd0&gt;] . &#44618;&#51008;&#49888;&#44221;&#47581;--DNN&#51012; &#51060;&#50857;&#54620; &#54644;&#44208; . - 목표: 아래와 같은 벡터 ${ boldsymbol u}$를 만들어보자. . ${ boldsymbol u} = [u_1,u_2, dots,u_{2000}], quad u_i = begin{cases} 9x_i +4.5&amp; x_i &lt;0 -4.5x_i + 4.5&amp; x_i &gt;0 end{cases}$ . &#44733;&#51064; &#44536;&#47000;&#54532;&#47484; &#47564;&#46300;&#45716; &#48169;&#48277;1 . u = [9*xi+4.5 if xi &lt;0 else -4.5*xi+4.5 for xi in x.reshape(-1).tolist()] #tolist하면 list화 plt.plot(u,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe9689550&gt;] . &#44733;&#51064; &#44536;&#47000;&#54532;&#47484; &#47564;&#46300;&#45716; &#48169;&#48277;2 . - 전략: 선형변환 $ to$ ReLU $ to$ 선형변환 . (예비학습) ReLU 함수란? . $ReLU(x) = max(0,x)$ . relu=torch.nn.ReLU() plt.plot(x,&#39;--r&#39;) plt.plot(relu(x),&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe697ad50&gt;] . 빨간색: x, 파란색: relu(x) | . 예비학습끝 . 우리 전략 다시 확인: 선형변환1 -&gt; 렐루 -&gt; 선형변환2 . (선형변환1) . plt.plot(x);plt.plot(-x) . [&lt;matplotlib.lines.Line2D at 0x7f4fe6902710&gt;] . (렐루) . plt.plot(x,alpha=0.2);plt.plot(-x,alpha=0.5) plt.plot(relu(x),&#39;--&#39;,color=&#39;C0&#39;);plt.plot(relu(-x),&#39;--&#39;,color=&#39;C1&#39;) #out feature을 2로 잡는당-&gt;선을 두개로 . [&lt;matplotlib.lines.Line2D at 0x7f4fe6884f50&gt;] . (선형변환2) . plt.plot(x,alpha=0.2);plt.plot(-x,alpha=0.2) plt.plot(relu(x),&#39;--&#39;,color=&#39;C0&#39;,alpha=0.2);plt.plot(relu(-x),&#39;--&#39;,color=&#39;C1&#39;,alpha=0.2) plt.plot(-4.5*relu(x)-9.0*relu(-x)+4.5,&#39;--&#39;,color=&#39;C2&#39;) #하늘색 점선과 노란색 점섬을 더해보자.. . [&lt;matplotlib.lines.Line2D at 0x7f4fe67922d0&gt;] . 이제 초록색선에 sig를 취하기만 하면? . plt.plot(sig(-4.5*relu(x)-9.0*relu(-x)+4.5),&#39;--&#39;,color=&#39;C2&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe6700650&gt;] . 정리하면! . fig = plt.figure(figsize=(8, 4)) spec = fig.add_gridspec(4, 4) ax1 = fig.add_subplot(spec[:2,0]); ax1.set_title(&#39;x&#39;); ax1.plot(x,&#39;--&#39;,color=&#39;C0&#39;) ax2 = fig.add_subplot(spec[2:,0]); ax2.set_title(&#39;-x&#39;); ax2.plot(-x,&#39;--&#39;,color=&#39;C1&#39;) ax3 = fig.add_subplot(spec[:2,1]); ax3.set_title(&#39;relu(x)&#39;); ax3.plot(relu(x),&#39;--&#39;,color=&#39;C0&#39;) ax4 = fig.add_subplot(spec[2:,1]); ax4.set_title(&#39;relu(-x)&#39;); ax4.plot(relu(-x),&#39;--&#39;,color=&#39;C1&#39;) ax5 = fig.add_subplot(spec[1:3,2]); ax5.set_title(&#39;u&#39;); ax5.plot(-4.5*relu(x)-9*relu(-x)+4.5,&#39;--&#39;,color=&#39;C2&#39;) ax6 = fig.add_subplot(spec[1:3,3]); ax6.set_title(&#39;yhat&#39;); ax6.plot(sig(-4.5*relu(x)-9*relu(-x)+4.5),&#39;--&#39;,color=&#39;C2&#39;) fig.tight_layout() . 이런느낌으로 $ hat{ boldsymbol y}$을 만들면 된다. | . torch.nn.Linear()&#47484; &#51060;&#50857;&#54620; &#44733;&#51064; &#44536;&#47000;&#54532; &#44396;&#54788; . torch.manual_seed(43052) l1 = torch.nn.Linear(in_features=1,out_features=2,bias=True) a1 = torch.nn.ReLU() l2 = torch.nn.Linear(in_features=2,out_features=1,bias=True) a2 = torch.nn.Sigmoid() . net = torch.nn.Sequential(l1,a1,l2,a2) . l1.weight,l1.bias,l2.weight,l2.bias . (Parameter containing: tensor([[-0.3467], [-0.8470]], requires_grad=True), Parameter containing: tensor([0.3604, 0.9336], requires_grad=True), Parameter containing: tensor([[ 0.2880, -0.6282]], requires_grad=True), Parameter containing: tensor([0.2304], requires_grad=True)) . l1.weight.data = torch.tensor([[1.0],[-1.0]]) l1.bias.data = torch.tensor([0.0, 0.0]) l2.weight.data = torch.tensor([[ -4.5, -9.0]]) l2.bias.data= torch.tensor([4.5]) l1.weight,l1.bias,l2.weight,l2.bias . (Parameter containing: tensor([[ 1.], [-1.]], requires_grad=True), Parameter containing: tensor([0., 0.], requires_grad=True), Parameter containing: tensor([[-4.5000, -9.0000]], requires_grad=True), Parameter containing: tensor([4.5000], requires_grad=True)) . plt.plot(l1(x).data) . [&lt;matplotlib.lines.Line2D at 0x7f4fe64e0d50&gt;, &lt;matplotlib.lines.Line2D at 0x7f4fe64e0f90&gt;] . plt.plot(a1(l1(x)).data) . [&lt;matplotlib.lines.Line2D at 0x7f4fe64608d0&gt;, &lt;matplotlib.lines.Line2D at 0x7f4fe6460b10&gt;] . plt.plot(l2(a1(l1(x))).data,color=&#39;C2&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe63e3150&gt;] . plt.plot(a2(l2(a1(l1(x)))).data,color=&#39;C2&#39;) #plt.plot(net(x).data,color=&#39;C2&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe634fa50&gt;] . - 수식표현 . ${ bf X}= begin{bmatrix} x_1 dots x_n end{bmatrix}$ . | $l_1({ bf X})={ bf X}{ bf W}^{(1)} overset{bc}{+} { boldsymbol b}^{(1)}= begin{bmatrix} x_1 &amp; -x_1 x_2 &amp; -x_2 dots &amp; dots x_n &amp; -x_n end{bmatrix}$ . ${ bf W}^{(1)}= begin{bmatrix} 1 &amp; -1 end{bmatrix}$ | ${ boldsymbol b}^{(1)}= begin{bmatrix} 0 &amp; 0 end{bmatrix}$ | . | $(a_1 circ l_1)({ bf X})= text{relu} big({ bf X}{ bf W}^{(1)} overset{bc}{+}{ boldsymbol b}^{(1)} big)= begin{bmatrix} text{relu}(x_1) &amp; text{relu}(-x_1) text{relu}(x_2) &amp; text{relu}(-x_2) dots &amp; dots text{relu}(x_n) &amp; text{relu}(-x_n) end{bmatrix}$ . | $(l_2 circ a_1 circ l_1)({ bf X})= text{relu} big({ bf X}{ bf W}^{(1)} overset{bc}{+}{ boldsymbol b}^{(1)} big){ bf W}^{(2)} overset{bc}{+}b^{(2)} = begin{bmatrix} -4.5 times text{relu}(x_1) -9.0 times text{relu}(-x_1) +4.5 -4.5 times text{relu}(x_2) -9.0 times text{relu}(-x_2) + 4.5 dots -4.5 times text{relu}(x_n) -9.0 times text{relu}(-x_n)+4.5 end{bmatrix}$ . ${ bf W}^{(2)}= begin{bmatrix} -4.5 -9 end{bmatrix}$ | $b^{(2)}=4.5$ | . | $net({ bf X})=(a_2 circ l_2 circ a_1 circ l_1)({ bf X})= text{sig} Big( text{relu} big({ bf X}{ bf W}^{(1)} overset{bc}{+}{ boldsymbol b}^{(1)} big){ bf W}^{(2)} overset{bc}{+}b^{(2)} Big) = begin{bmatrix} text{sig} Big(-4.5 times text{relu}(x_1) -9.0 times text{relu}(-x_1) +4.5 Big) text{sig} Big(-4.5 times text{relu}(x_2) -9.0 times text{relu}(-x_2) + 4.5 Big) dots text{sig} Big(-4.5 times text{relu}(x_n) -9.0 times text{relu}(-x_n)+4.5 Big) end{bmatrix}$ . | - 차원만 따지자 . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,2)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,2)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcb4b0bcd50&gt;] . Step1 ~ Step4 . - 준비 . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=2), #u1=l1(x), x:(n,1) --&gt; u1:(n,2) torch.nn.ReLU(), # v1=a1(u1), u1:(n,2) --&gt; v1:(n,2) torch.nn.Linear(in_features=2,out_features=1), # u2=l2(v1), v1:(n,2) --&gt; u2:(n,1) torch.nn.Sigmoid() # v2=a2(u2), u2:(n,1) --&gt; v2:(n,1) ) . loss_fn = torch.nn.BCELoss() . optimizr = torch.optim.Adam(net.parameters()) # lr은 디폴트값으로.. . - 반복 . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) plt.title(&quot;before&quot;) . Text(0.5, 1.0, &#39;before&#39;) . for epoc in range(3000): ## step1 yhat = net(x) ## step2 loss = loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;,color=&#39;C1&#39;) plt.title(&quot;after 3000 epochs&quot;) . Text(0.5, 1.0, &#39;after 3000 epochs&#39;) . for epoc in range(3000): ## step1 yhat = net(x) ## step2 loss = loss_fn(yhat,y) ## step3 loss.backward() ## step4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;,color=&#39;C1&#39;) plt.title(&quot;after 6000 epochs&quot;) . Text(0.5, 1.0, &#39;after 6000 epochs&#39;) . &#44618;&#51008;&#49888;&#44221;&#47581;--DNN&#51004;&#47196; &#54644;&#44208;&#44032;&#45733;&#54620; &#45796;&#50577;&#54620; &#50696;&#51228; . &#50696;&#51228;1 . - 언뜻 생각하면 방금 배운 기술은 sig를 취하기 전이 꺽은선인 형태만 가능할 듯 하다. $ to$ 그래서 이 역시 표현력이 부족할 듯 하다. $ to$ 그런데 생각보다 표현력이 풍부한 편이다. 즉 생각보다 쓸 만하다. . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex1.csv&#39;) . x = torch.tensor(df.x).float().reshape(-1,1) y = torch.tensor(df.y).float().reshape(-1,1) . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4fe62d7850&gt;] . 이거 시그모이드 취하기 직전은 step이 포함된 듯 $ to$ 그래서 꺽은선으로는 표현할 수 없는 구조임 $ to$ 그런데 사실 대충은 표현가능 | . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=16), # x:(n,1) --&gt; u1:(n,16) #최대 16번 꺾일 수 있음.. torch.nn.ReLU(), # u1:(n,16) --&gt; v1:(n,16) torch.nn.Linear(in_features=16,out_features=1), # v1:(n,16) --&gt; u2:(n,1) torch.nn.Sigmoid() # u2:(n,1) --&gt; v2:(n,1) ) . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,16)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,16)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ | . loss_fn = torch.nn.BCELoss() . optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(6000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) #실제로는 관츷ㄱ 못하는거 plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcb40d64ed0&gt;] . &#50696;&#51228;2 . - 사실 꺽은선의 조합으로 꽤 많은걸 표현할 수 있거든요? $ to$ 심지어 곡선도 대충 맞게 적합된다. . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex2.csv&#39;) . x = torch.tensor(df.x).float().reshape(-1,1) y = torch.tensor(df.y).float().reshape(-1,1) . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcb3ddfd210&gt;] . x=torch.tensor(df.x).float().reshape(-1,1) y=torch.tensor(df.y).float().reshape(-1,1) . (풀이1) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --&gt; u1:(n,32) torch.nn.ReLU(), # u1:(n,32) --&gt; v1:(n,32) torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --&gt; u2:(n,1) ) . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,32)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,32)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ | . loss_fn = torch.nn.MSELoss() #mseloss:마지막에 sigmoid형태가 아니니까! . optimizr = torch.optim.Adam(net.parameters()) . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcb1c74e990&gt;] . for epoc in range(6000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,lw=4) #lw:두겁게 . [&lt;matplotlib.lines.Line2D at 0x7fcb1c785150&gt;] . (풀이2) -- 풀이1보다 좀 더 잘맞음. 잘 맞는 이유? 좋은초기값 (=운) . torch.manual_seed(5) # seed값을 43052-&gt;5로바꿔주기... net = torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --&gt; u1:(n,32) torch.nn.ReLU(), # u1:(n,32) --&gt; v1:(n,32) torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --&gt; u2:(n,1) ) . $ underset{(n,1)}{ bf X} overset{l_1}{ to} underset{(n,32)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,32)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ | . loss_fn = torch.nn.MSELoss() . optimizr = torch.optim.Adam(net.parameters()) . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fcb1c3f9e50&gt;] . for epoc in range(6000): ## 1 yhat = net(x) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . plt.plot(x,y,&#39;o&#39;,alpha=0.02) plt.plot(df.x,df.underlying,&#39;-b&#39;) plt.plot(x,net(x).data,lw=4) . [&lt;matplotlib.lines.Line2D at 0x7fcb1bfbda50&gt;] . 풀이1에서 에폭을 많이 반복하면 풀이2의 적합선이 나올까? --&gt; 안나옴!! (local min에 빠졌다) | . &#50696;&#51228;3 . import seaborn as sns . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex3.csv&#39;) df . x1 x2 y . 0 -0.874139 | 0.210035 | 0.0 | . 1 -1.143622 | -0.835728 | 1.0 | . 2 -0.383906 | -0.027954 | 0.0 | . 3 2.131652 | 0.748879 | 1.0 | . 4 2.411805 | 0.925588 | 1.0 | . ... ... | ... | ... | . 1995 -0.002797 | -0.040410 | 0.0 | . 1996 -1.003506 | 1.182736 | 0.0 | . 1997 1.388121 | 0.079317 | 0.0 | . 1998 0.080463 | 0.816024 | 1.0 | . 1999 -0.416859 | 0.067907 | 0.0 | . 2000 rows × 3 columns . sns.scatterplot(data=df,x=&#39;x1&#39;,y=&#39;x2&#39;,hue=&#39;y&#39;,alpha=0.5,palette={0:(0.5, 0.0, 1.0),1:(1.0,0.0,0.0)}) . &lt;AxesSubplot:xlabel=&#39;x1&#39;, ylabel=&#39;x2&#39;&gt; . x1 = torch.tensor(df.x1).float().reshape(-1,1) x2 = torch.tensor(df.x2).float().reshape(-1,1) X = torch.concat([x1,x2],axis=1) y = torch.tensor(df.y).float().reshape(-1,1) . X.shape . torch.Size([2000, 2]) . torch.manual_seed(43052) net = torch.nn.Sequential( torch.nn.Linear(in_features=2,out_features=32),#X의 shape이 2니까 in_features=2 torch.nn.ReLU(), torch.nn.Linear(in_features=32,out_features=1), torch.nn.Sigmoid() ) . $ underset{(n,2)}{ bf X} overset{l_1}{ to} underset{(n,32)}{ boldsymbol u^{(1)}} overset{a_1}{ to} underset{(n,32)}{ boldsymbol v^{(1)}} overset{l_2}{ to} underset{(n,1)}{ boldsymbol u^{(2)}} overset{a_2}{ to} underset{(n,1)}{ boldsymbol v^{(2)}}= underset{(n,1)}{ hat{ boldsymbol y}}$ | . loss_fn = torch.nn.BCELoss() . optimizr = torch.optim.Adam(net.parameters()) . for epoc in range(3000): ## 1 yhat = net(X) ## 2 loss = loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizr.step() optimizr.zero_grad() . df2 = df.assign(yhat=yhat.reshape(-1).detach().tolist()) #seaborn을 그리려먼 dataframe형식으로 되어잇어야해 df2 . x1 x2 y yhat . 0 -0.874139 | 0.210035 | 0.0 | 0.345833 | . 1 -1.143622 | -0.835728 | 1.0 | 0.605130 | . 2 -0.383906 | -0.027954 | 0.0 | 0.111915 | . 3 2.131652 | 0.748879 | 1.0 | 0.918491 | . 4 2.411805 | 0.925588 | 1.0 | 0.912608 | . ... ... | ... | ... | ... | . 1995 -0.002797 | -0.040410 | 0.0 | 0.254190 | . 1996 -1.003506 | 1.182736 | 0.0 | 0.508002 | . 1997 1.388121 | 0.079317 | 0.0 | 0.410099 | . 1998 0.080463 | 0.816024 | 1.0 | 0.262315 | . 1999 -0.416859 | 0.067907 | 0.0 | 0.107903 | . 2000 rows × 4 columns . sns.scatterplot(data=df2,x=&#39;x1&#39;,y=&#39;x2&#39;,hue=&#39;yhat&#39;,alpha=0.5,palette=&#39;rainbow&#39;) . &lt;AxesSubplot:xlabel=&#39;x1&#39;, ylabel=&#39;x2&#39;&gt; . - 결과시각화 . fig, ax = plt.subplots(1,2,figsize=(8,4)) sns.scatterplot(data=df,x=&#39;x1&#39;,y=&#39;x2&#39;,hue=&#39;y&#39;,alpha=0.5,palette={0:(0.5, 0.0, 1.0),1:(1.0,0.0,0.0)},ax=ax[0]) sns.scatterplot(data=df2,x=&#39;x1&#39;,y=&#39;x2&#39;,hue=&#39;yhat&#39;,alpha=0.5,palette=&#39;rainbow&#39;,ax=ax[1]) . &lt;AxesSubplot:xlabel=&#39;x1&#39;, ylabel=&#39;x2&#39;&gt; . - 교훈: underlying이 엄청 이상해보여도 생각보다 잘 맞춤 .",
            "url": "https://boram-coco.github.io/coco1/2022/09/28/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9405%EC%9D%BC.html",
            "relUrl": "/2022/09/28/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9405%EC%9D%BC.html",
            "date": " • Sep 28, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "기계학습특강",
            "content": "# 우리의 2차 목표: 그 모형에 &quot;새로운&quot; 자료를 전달하여 이미지를 분류할 것이다. (즉 클래시파이어를 쓴다는 소리) . from fastai.vision.all import * . path=untar_data(URLs.PETS)/&#39;images&#39; . . 100.00% [811712512/811706944 01:16&lt;00:00] path . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;) . PILImage.create(&#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39;) . _lst = &#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39;,&#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_10.jpg&#39; . _lst[0] . &#39;/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39; . faaaa = get_image_files(path) #교수님은 filenames로 설정함 . faaaa[0] . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/leonberger_137.jpg&#39;) . PILImage.create(&#39;/root/.fastai/data/oxford-iiit-pet/images/newfoundland_28.jpg&#39;) . PILImage.create(faaaa[0]) . print(faaaa[1]) PILImage.create(faaaa[1]) . /root/.fastai/data/oxford-iiit-pet/images/Birman_139.jpg . print(faaaa[3]) PILImage.create(faaaa[3]) . /root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_148.jpg . print(faaaa[6]) PILImage.create(faaaa[6]) . /root/.fastai/data/oxford-iiit-pet/images/Siamese_79.jpg . &#39;A&#39;.isupper() . True . &#39;abdjlkfwe.jpg&#39;[0] . &#39;a&#39; . def f(fname): if fname[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . dls = ImageDataLoaders.from_name_func(path,faaaa,f,item_tfms=Resize(224)) . dls.show_batch(max_n=25) . # 우리의 2차 목표: 그 모형에 &quot;새로운&quot; 자료를 전달하여 이미지를 분류할 것이다. (즉 클래시파이어를 쓴다는 소리) ## 오브젝트.. 오브젝트에는 동사와 명사 가 있어요 ### 명사 # (1) 데이터 # (2) 채용한 모형의 이론 # (3) 평가기준 matric ### 동사 # (1) 학습 # (2) 판단 . ysj = cnn_learner(dls,resnet34,metrics=error_rate) ##저항률 확인(잘 파악하는지 확인하기 위해서 metrics=error_rate 이용) ysj.fine_tune(1) ## 3을 쓰면 1보다는 많이 한다는 뜻 ## 학습하다..동사,, . /usr/local/lib/python3.7/dist-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code warn(&#34;`cnn_learner` has been renamed to `vision_learner` -- please update your code&#34;) /usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter &#39;pretrained&#39; is deprecated since 0.13 and will be removed in 0.15, please use &#39;weights&#39; instead. f&#34;The parameter &#39;{pretrained_param}&#39; is deprecated since 0.13 and will be removed in 0.15, &#34; /usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg) Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth . epoch train_loss valid_loss error_rate time . 0 | 0.147350 | 0.014042 | 0.004060 | 00:56 | . epoch train_loss valid_loss error_rate time . 0 | 0.053051 | 0.012090 | 0.004736 | 00:52 | . ?cnn_learner . PILImage.create(faaaa[1]) . ysj.predict(PILImage.create(faaaa[1])) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 3.0773e-10])) . ysj.predict(PILImage.create(faaaa[6])) . &#46041;&#49324;&#51060;&#44256;.. &#46244;&#50640; &#51216;&#52237;&#50632;&#51004;&#45768;&#44620; &#54632;&#49688;&#45796; &#49373;&#44033;&#54616;&#44592; . &#51077;&#47141;&#51012; &#51060;&#48120;&#51648; &#51088;&#52404;&#47196; &#45347;&#50632;&#45716;&#45936;, &#51060;&#48120;&#51648;&#44032; &#51200;&#51109;&#46108; path&#47564; &#45347;&#50612;&#46020; &#46104;&#51648;&#50506;&#51012;&#44620;? . ysj.predict(faaaa[6]) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 2.2963e-10])) . ysj.show_results() . # 체크를 하는 object를 만들어야함 checker = Interpretation.from_learner(ysj) . checker.plot_top_losses(16) # 첫번째 사진에서 5.59는 로스이고 1퍼의 확률로 강아지라고 생각함 # 로스는 몇퍼의 확률로 잘못생각했느냐에 따라서 달라질 수 있음 # 맞추는 걸 넘어서 확실해야 로스가 적다. (확신의여부) # 오버피팅 아냐..? 과대적합..? 자기들이 이미 다학습된 내용 가지고 보여주는거아냐? 생각-&gt;새로운 이미지 부여 . img=PILImage.create(requests.get(&#39;https://dimg.donga.com/ugc/CDB/SHINDONGA/Article/5e/0d/9f/01/5e0d9f011a9ad2738de6.jpg&#39;).content) ysj.predict(img) . (&#39;dog&#39;, TensorBase(1), TensorBase([2.8106e-06, 1.0000e+00])) . img=PILImage.create(requests.get(&#39;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcnSq1X%2Fbtq4o9AdWTH%2FHTm9TZG4AszSwLPFlVfGW0%2Fimg.jpg&#39;).content) ysj.predict(img) . (&#39;dog&#39;, TensorBase(1), TensorBase([1.0909e-06, 1.0000e+00])) . img=PILImage.create(requests.get(&#39;https://image.edaily.co.kr/images/photo/files/NP/S/2022/04/PS22042501396.jpg&#39;).content) ysj.predict(img) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 2.6542e-12])) . img=PILImage.create(requests.get(&#39;https://blog.kakaocdn.net/dn/zfQQi/btrydI0vGzm/3YY3KrPEwKN558e27H6t0k/img.jpg&#39;).content) ysj.predict(img) . (&#39;cat&#39;, TensorBase(0), TensorBase([0.9805, 0.0195])) . PILImage.create(&#39;/강아지사진1.jpg&#39;) .",
            "url": "https://boram-coco.github.io/coco1/2022/09/07/ML.html",
            "relUrl": "/2022/09/07/ML.html",
            "date": " • Sep 7, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://boram-coco.github.io/coco1/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://boram-coco.github.io/coco1/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "about me에 대한 관련된 내용 작성 . 프로젝트(수업들으면서 했던 프로젝트를 쓰든지,,) . 신기하다 ㅇ와 . 시계열분석을 활용한 삼ㅈ성전자 주식예측(2022-05) . 이런식으로… . 프로그래밍 스킬 ( R 파이썬 .. ) .",
          "url": "https://boram-coco.github.io/coco1/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://boram-coco.github.io/coco1/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}